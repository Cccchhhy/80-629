{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example that trains two Naive Bayes classifiers and explore some of the resulting model\n",
    "#### Code adapted from:\n",
    "http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Usage: __main__.py [options]\n",
      "\n",
      "Options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --report              Print a detailed classification report.\n",
      "  --chi2_select=SELECT_CHI2\n",
      "                        Select some number of features using a chi-squared\n",
      "                        test\n",
      "  --confusion_matrix    Print the confusion matrix.\n",
      "  --top10               Print ten most discriminative terms per class for\n",
      "                        every classifier.\n",
      "  --all_categories      Whether to use all categories or not.\n",
      "  --use_hashing         Use a hashing vectorizer.\n",
      "  --n_features=N_FEATURES\n",
      "                        n_features when using the hashing vectorizer.\n",
      "  --filtered            Remove newsgroup information that is easily overfit:\n",
      "                        headers, signatures, and quoting.\n",
      "()\n",
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
      "data loaded\n",
      "2034 documents - 3.980MB (training set)\n",
      "1353 documents - 2.867MB (test set)\n",
      "4 categories\n",
      "()\n",
      "Extracting features from the training data using a sparse vectorizer\n",
      "done in 0.962815s at 4.133MB/s\n",
      "n_samples: 2034, n_features: 1000\n",
      "()\n",
      "Extracting features from the test data using the same vectorizer\n",
      "done in 0.636881s at 4.502MB/s\n",
      "n_samples: 1353, n_features: 1000\n",
      "()\n",
      "================================================================================\n",
      "Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "train time: 0.003s\n",
      "test time:  0.001s\n",
      "accuracy:   0.849\n",
      "dimensionality: 1000\n",
      "density: 1.000000\n",
      "()\n",
      "()\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "train time: 0.003s\n",
      "test time:  0.002s\n",
      "accuracy:   0.812\n",
      "dimensionality: 1000\n",
      "density: 1.000000\n",
      "()\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#         Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Mathieu Blondel <mathieu@mblondel.org>\n",
    "#         Lars Buitinck\n",
    "# License: BSD 3 clause\n",
    "\n",
    "#from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "\n",
    "# parse commandline arguments\n",
    "op = OptionParser()\n",
    "op.add_option(\"--report\",\n",
    "              action=\"store_true\", dest=\"print_report\",\n",
    "              help=\"Print a detailed classification report.\")\n",
    "op.add_option(\"--chi2_select\",\n",
    "              action=\"store\", type=\"int\", dest=\"select_chi2\",\n",
    "              help=\"Select some number of features using a chi-squared test\")\n",
    "op.add_option(\"--confusion_matrix\",\n",
    "              action=\"store_true\", dest=\"print_cm\",\n",
    "              help=\"Print the confusion matrix.\")\n",
    "op.add_option(\"--top10\",\n",
    "              action=\"store_true\", dest=\"print_top10\",\n",
    "              help=\"Print ten most discriminative terms per class\"\n",
    "                   \" for every classifier.\")\n",
    "op.add_option(\"--all_categories\",\n",
    "              action=\"store_true\", dest=\"all_categories\",\n",
    "              help=\"Whether to use all categories or not.\")\n",
    "op.add_option(\"--use_hashing\",\n",
    "              action=\"store_false\",\n",
    "              help=\"Use a hashing vectorizer.\")\n",
    "op.add_option(\"--n_features\",\n",
    "              action=\"store\", type=int, default=10**3,\n",
    "              help=\"n_features when using the hashing vectorizer.\")\n",
    "op.add_option(\"--filtered\",\n",
    "              action=\"store_true\",\n",
    "              help=\"Remove newsgroup information that is easily overfit: \"\n",
    "                   \"headers, signatures, and quoting.\")\n",
    "\n",
    "\n",
    "def is_interactive():\n",
    "    return not hasattr(sys.modules['__main__'], '__file__')\n",
    "\n",
    "# work-around for Jupyter notebook and IPython console\n",
    "argv = [] if is_interactive() else sys.argv[1:]\n",
    "(opts, args) = op.parse_args(argv)\n",
    "if len(args) > 0:\n",
    "    op.error(\"this script takes no arguments.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(__doc__)\n",
    "op.print_help()\n",
    "print()\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Load some categories from the training set\n",
    "if opts.all_categories:\n",
    "    categories = None\n",
    "else:\n",
    "    categories = [\n",
    "        'alt.atheism',\n",
    "        'talk.religion.misc',\n",
    "        'comp.graphics',\n",
    "        'sci.space',\n",
    "    ]\n",
    "\n",
    "if opts.filtered:\n",
    "    remove = ('headers', 'footers', 'quotes')\n",
    "else:\n",
    "    remove = ()\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories if categories else \"all\")\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "print('data loaded')\n",
    "\n",
    "# order of labels in `target_names` can be different from `categories`\n",
    "target_names = data_train.target_names\n",
    "\n",
    "\n",
    "def size_mb(docs):\n",
    "    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
    "\n",
    "data_train_size_mb = size_mb(data_train.data)\n",
    "data_test_size_mb = size_mb(data_test.data)\n",
    "\n",
    "print(\"%d documents - %0.3fMB (training set)\" % (\n",
    "    len(data_train.data), data_train_size_mb))\n",
    "print(\"%d documents - %0.3fMB (test set)\" % (\n",
    "    len(data_test.data), data_test_size_mb))\n",
    "print(\"%d categories\" % len(categories))\n",
    "print()\n",
    "\n",
    "# split a training set and a test set\n",
    "y_train, y_test = data_train.target, data_test.target\n",
    "\n",
    "print(\"Extracting features from the training data using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "if opts.use_hashing:\n",
    "    vectorizer = HashingVectorizer(stop_words='english', alternate_sign=False,\n",
    "                                   n_features=opts.n_features)\n",
    "    X_train = vectorizer.transform(data_train.data)\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                                 stop_words='english', max_features=1000)\n",
    "    X_train = vectorizer.fit_transform(data_train.data)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_train_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "print()\n",
    "\n",
    "print(\"Extracting features from the test data using the same vectorizer\")\n",
    "t0 = time()\n",
    "X_test = vectorizer.transform(data_test.data)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_test_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "print()\n",
    "\n",
    "# mapping from integer feature name to original token string\n",
    "if opts.use_hashing:\n",
    "    feature_names = None\n",
    "else:\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "if opts.select_chi2:\n",
    "    print(\"Extracting %d best features by a chi-squared test\" %\n",
    "          opts.select_chi2)\n",
    "    t0 = time()\n",
    "    ch2 = SelectKBest(chi2, k=opts.select_chi2)\n",
    "    X_train = ch2.fit_transform(X_train, y_train)\n",
    "    X_test = ch2.transform(X_test)\n",
    "    if feature_names:\n",
    "        # keep selected feature names\n",
    "        feature_names = [feature_names[i] for i\n",
    "                         in ch2.get_support(indices=True)]\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "if feature_names:\n",
    "    feature_names = np.asarray(feature_names)\n",
    "\n",
    "\n",
    "def trim(s):\n",
    "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\"\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Benchmark classifiers\n",
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "        if opts.print_top10 and feature_names is not None:\n",
    "            print(\"top 10 keywords per class:\")\n",
    "            for i, label in enumerate(target_names):\n",
    "                top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "                print(trim(\"%s: %s\" % (label, \" \".join(feature_names[top10]))))\n",
    "        print()\n",
    "\n",
    "    if opts.print_report:\n",
    "        print(\"classification report:\")\n",
    "        print(metrics.classification_report(y_test, pred,\n",
    "                                            target_names=target_names))\n",
    "\n",
    "    if opts.print_cm:\n",
    "        print(\"confusion matrix:\")\n",
    "        print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "# Train sparse Naive Bayes classifiers\n",
    "print('=' * 80)\n",
    "print(\"Naive Bayes\")\n",
    "results.append(benchmark(MultinomialNB(alpha=.01)))\n",
    "results.append(benchmark(BernoulliNB(alpha=.01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mNB = MultinomialNB()\n",
    "mNB.fit(X_train, y_train)\n",
    "preds_score = mNB.predict(X_test)\n",
    "preds_prob = mNB.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MESSAGE 22\n",
      "===========\n",
      "\n",
      "\n",
      "From: mikec@sail.LABS.TEK.COM (Micheal Cranford)\n",
      "Subject: the nature of light\n",
      "Organization: Tektronix, Inc., Beaverton,  OR.\n",
      "Lines: 21\n",
      "\n",
      "In article <30185@ursa.bear.com> halat@pooh.bears (Jim Halat) writes:\n",
      "[ deleted ]\n",
      ">Take light as another example.  There are two theories: particle and\n",
      ">wave.  Each one fails to predict the behavior of light as some point.\n",
      ">So which is it: particle or wave?\n",
      "[ deleted ]\n",
      "\n",
      "  Your information on this topic is very much out of date.  Quantum Electro-\n",
      "dynamics (QED - which considers light to be particles) has been experimentally\n",
      "verified to about 14 decimal digits of precision under ALL tested conditions.\n",
      "I'm afraid that this case, at least in the physics community, has been decided.\n",
      "Laymen should consult \"QED - The Strange Theory of Light and Matter\" by Richard\n",
      "P. Feynman and for the more technically minded there's \"The Feynman Lectures on\n",
      "Physics\" by Feynman, Leighton and Sands (an excellent 3 volumes).  Case closed.\n",
      "\n",
      "\n",
      "  UUCP:  uunet!tektronix!sail!mikec  or                  M.Cranford\n",
      "         uunet!tektronix!sail.labs.tek.com!mikec         Principal Troll\n",
      "  ARPA:  mikec%sail.LABS.TEK.COM@RELAY.CS.NET            Resident Skeptic\n",
      "  CSNet: mikec@sail.LABS.TEK.COM                         TekLabs, Tektronix\n",
      "\n",
      "\n",
      "===========\n",
      "\n",
      "PREDICTED CLASS: 3\n",
      "ACTUAL CLASS: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_id=22\n",
    "print(\"MESSAGE %d\\n===========\\n\\n\" % doc_id)\n",
    "print(data_test.data[doc_id])\n",
    "print(\"===========\\n\")\n",
    "print (\"PREDICTED CLASS: %d\" % y_test[doc_id])\n",
    "print (\"ACTUAL CLASS: %d\\n\" % preds_score[doc_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate words from a class. P(x | y = k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS: comp.graphics\n",
      "Number of features per class: 1000\n",
      "\n",
      "\n",
      "Top 20 words for class 1:\n",
      "\tgraphics (9e-03)\n",
      "\tthanks (6e-03)\n",
      "\tuniversity (6e-03)\n",
      "\tposting (6e-03)\n",
      "\thost (5e-03)\n",
      "\tnntp (5e-03)\n",
      "\tcom (5e-03)\n",
      "\tfiles (5e-03)\n",
      "\timage (5e-03)\n",
      "\tfile (5e-03)\n",
      "\thelp (5e-03)\n",
      "\tneed (4e-03)\n",
      "\tlooking (4e-03)\n",
      "\tprogram (4e-03)\n",
      "\t3d (4e-03)\n",
      "\tknow (4e-03)\n",
      "\tcomputer (4e-03)\n",
      "\twindows (4e-03)\n",
      "\tdoes (4e-03)\n",
      "\tmail (4e-03)\n",
      "\n",
      "\n",
      "\n",
      "Words generated from the class conditional distribution:\n",
      "\tlooking (4e-03)\n",
      "\tpolygon (3e-03)\n",
      "\tfile (5e-03)\n",
      "\taustralia (1e-03)\n",
      "\tstuff (1e-03)\n",
      "\tactually (1e-03)\n",
      "\tdon (3e-03)\n",
      "\tprograms (2e-03)\n",
      "\tideas (1e-03)\n",
      "\t30 (1e-03)\n",
      "\t35 (8e-04)\n",
      "\tinterface (1e-03)\n",
      "\tvehicle (2e-04)\n",
      "\tcode (4e-03)\n",
      "\tissue (6e-04)\n",
      "\twindows (4e-03)\n",
      "\tweek (1e-03)\n",
      "\trunning (2e-03)\n",
      "\timage (5e-03)\n",
      "\treasonable (6e-04)\n",
      "\tsaw (7e-04)\n",
      "\tcanada (1e-03)\n",
      "\tshuttle (3e-04)\n",
      "\tsolntze (2e-04)\n",
      "\tnetcom (8e-04)\n"
     ]
    }
   ],
   "source": [
    "k=1\n",
    "\n",
    "\n",
    "print(\"CLASS: %s\" % target_names[k])\n",
    "feature_prob = np.exp(mNB.feature_log_prob_)\n",
    "print(\"Number of features per class: %d\\n\\n\" % feature_prob.shape[1])\n",
    "\n",
    "topN = 20\n",
    "print(\"Top %d words for class %d:\" % (topN, k))\n",
    "for i in np.argsort(-feature_prob[k,:])[:topN]:\n",
    "    print '\\t', feature_names[i], '(%.0e)' % feature_prob[k,i]\n",
    "    \n",
    "print('\\n\\n')\n",
    "#print(feature_prob[1].flatten().shape)\n",
    "print(\"Words generated from the class conditional distribution:\")\n",
    "n_words =  25\n",
    "idx = np.random.choice(feature_prob.shape[1], size=n_words, replace=True, p=feature_prob[k].flatten())\n",
    "for i in xrange(idx.size):\n",
    "    print '\\t', feature_names[idx[i]], '(%.0e)' % feature_prob[k,idx[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words probabilities under different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_id = {}\n",
    "for i in xrange(feature_names.size):\n",
    "    word_to_id[feature_names[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category\t\tProb.\n",
      "========\t\t=====\n",
      "comp.graphics        \t3.2e-03\n",
      "talk.religion.misc   \t3.0e-04\n",
      "alt.atheism          \t2.5e-04\n",
      "sci.space            \t2.2e-04\n"
     ]
    }
   ],
   "source": [
    "word = 'vga'\n",
    "\n",
    "\n",
    "word_id = word_to_id[word]\n",
    "\n",
    "feature_prob = np.exp(mNB.feature_log_prob_)\n",
    "\n",
    "\n",
    "idx = np.argsort(-feature_prob[:,word_id])\n",
    "\n",
    "print('Category\\t\\tProb.')\n",
    "print('========\\t\\t=====')\n",
    "for cat in idx:\n",
    "    print '{:<20}'.format(target_names[cat]), \"\\t%.1e\" % feature_prob[cat,word_id]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
