{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH80629A\n",
    "# Week \\#7 - Unsupervised Learning - Exercises\n",
    "\n",
    "This tutorial will focus on two tasks which used unsupervised learning, namely clustering (Sections 7.2 and 7.3) and dimensionality reduction (Section 7). The goal of this tutorial is to develop basic intuition behind some classic algorithms used for unsupervised learning (k-means, GMMs, and autoencoders)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘utilities.py’ already there; not retrieving.\n",
      "\n",
      "mkdir: Images: File exists\n",
      "File ‘Images/AE.png’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data science libraries\n",
    "import sklearn as sk\n",
    "from sklearn.cluster import KMeans   # KMeans function\n",
    "from sklearn.datasets import make_circles, make_blobs  # Datasets\n",
    "from sklearn.model_selection import train_test_split   # Cross validation library\n",
    "from sklearn import mixture\n",
    "\n",
    "# Data visualization libaries\n",
    "import matplotlib.pyplot as plt\n",
    "# A must! For nice an easy figures - look for sns command in the notebook\n",
    "import seaborn as sns   \n",
    "from matplotlib.pyplot import cm   # This is a nice color chart\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Code to obtain utilities.py\n",
    "!wget -nc https://raw.githubusercontent.com/lcharlin/80-629/master/week7-Unsupervised/utilities.py\n",
    "!mkdir Images\n",
    "!wget -nc -P Images https://raw.githubusercontent.com/lcharlin/80-629/master/week7-Unsupervised/Images/AE.png\n",
    "\n",
    "# Homemade libraries\n",
    "from utilities import color, super_scat_it, distance, initiate, estimate_centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Data generation\n",
    "\n",
    "Let's first look at [Gaussian mixtures](https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model), a relatively simple model, which we will generate with the [make_blobs](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs) function. \n",
    "\n",
    "**Remark**: In order to properly run Section 7.2.1, do not change the attributes of the `make_blobs` function in the code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_obs = 1000   # Number of observation\n",
    "k = 2   # Number of clusters\n",
    "std = 4   # Standard deviation associated to the isotopic Gaussian Mixture - \n",
    "dim = 2   # Covariates dimension - if dim > 2, don't expect data vizualisation from matplotlib!\n",
    "seed = 10   # Random seed to replicate the experience\n",
    "\n",
    "X, y = make_blobs(n_samples=nb_obs, centers=k, cluster_std= std, n_features=dim, random_state=seed)   # Data generation\n",
    "\n",
    "super_scat_it(X, y, k)   # Data visualization - see utilities.py script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Clustering: K-means Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 Implementation with numpy\n",
    "\n",
    "The objective of this section is to implement and understand the procedures associated with the k-means algorithm. First, we will therefore implement the k-means algorithm with numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7.1**\n",
    "\n",
    "According to the pseudo code presented on [Slide 18 of the course](http://www.cs.toronto.edu/~lcharlin/courses/80-629/slides_unsupervised.pdf), complete the `fit` function of the `k_means` class below.\n",
    "\n",
    "**Answer 7.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class k_means:\n",
    "\n",
    "    def __init__(self, data, k, seed=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: unlabeled data\n",
    "            k: number of cluster\n",
    "        Class Attributes:\n",
    "            self.data: unlabeled data\n",
    "            self.centroid: cluster centers\n",
    "            self.label: label \n",
    "            self.iteration: number of iteration before k-means converges\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data = data\n",
    "        self.centroid = initiate(data, k)\n",
    "        self.label = np.argmin(distance(self.data, self.centroid ), axis=1)\n",
    "            \n",
    "    def fit(self):\n",
    "        \n",
    "        # step 1. update the cluster centers\n",
    "        self.centroid = estimate_centroid(self.data, self.label)\n",
    "        # step 2. update the responsibilities (i.e., the cluster each point belongs to)\n",
    "        label_new = np.argmin(distance(self.data, self.centroid), axis=1)\n",
    "        \n",
    "        # run both steps until convergence        \n",
    "        while label_new.tolist() != self.label.tolist():\n",
    "            self.label = label_new\n",
    "            # step 1.\n",
    "            # step 2.\n",
    "                        \n",
    "        self.label = label_new\n",
    "        # compute the objective function\n",
    "        self.objective = np.mean(np.min(distance(self.data, self.centroid), axis=1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call the above `k_means` class, estimate the centroids and visualize the associated clusters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "km = k_means(X, k)\n",
    "km.fit()\n",
    "\n",
    "super_scat_it(X, km.label, dim, km.centroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 Exploration of the K-means Algorithm with Scikit Learn\n",
    "\n",
    "Once the algorithm has been coded, we are going to make our life easier and simply use the [Scikit Learn library](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) (-_-).  First, let's check that everything is running fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(X)\n",
    "super_scat_it(X, kmeans.labels_, dim, kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3 Choosing the optimal number of clusters\n",
    "\n",
    "In our first experiment, we knew the actual number of subpopulations (parameterized by the variable $ k $) associated with the simulated data. On the other hand, with non simulated datasets, the data is only very rarely labeled. It is therefore important to develop methodologies in order to clearly define the number of clusters required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions 7.2**\n",
    "\n",
    "1. Find a simple way to determine the optimal number of clusters.\n",
    "2. Implement it.\n",
    "3. How many clusters would you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions 7.3**\n",
    "\n",
    "   1. Are you disappointed by the behaviour of the curve associated to the validation set?\n",
    "   2. Considering the results obtained here, could you imagine a better way to know the optimal number of clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous exercise was based on a relatively simple dataset. Indeed, there was a large number of observations ($n = 1000$) for a relatively small variable space ($\\bf{X} \\in \\mathbb{R}^2$) and small number of clusters ($k = 2$). In order to validate the relevance of the cross-validation procedure to fix the number of clusters, let's now simulate a slightly more complex dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_obs = 100   # Number of observation\n",
    "k = 10   # Number of clusters\n",
    "std = 4   # Standard deviation associated to the isotopic Gaussian Mixture - \n",
    "dim = 50   # Covariates dimension - if dim > 2\n",
    "           # (don't expect data vizualisation from matplotlib)\n",
    "seed = 10   # Random seed to replicate the experience\n",
    "\n",
    "X, y = make_blobs(n_samples=nb_obs, centers=k, cluster_std= std, \n",
    "                  n_features=dim, random_state=seed)   # Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look how the loss associated to the validation set behave on a more complex dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, _, _ = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "loss_train, loss_valid = [], []\n",
    "\n",
    "max_cluster = 50\n",
    "\n",
    "for k in np.arange(max_cluster)+1:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(X_train)\n",
    "    loss_train.append(np.mean(np.min(distance(X_train, kmeans.cluster_centers_), axis=1)))\n",
    "    loss_valid.append(np.mean(np.min(distance(X_valid, kmeans.cluster_centers_), axis=1)))\n",
    "\n",
    "plt.plot(np.arange(max_cluster)+1, loss_train, label='Train')\n",
    "plt.plot(np.arange(max_cluster)+1, loss_valid, label='Validation')\n",
    "\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Ghosting the legend\n",
    "leg = plt.gca().legend(loc='center left', bbox_to_anchor=(1, .85))\n",
    "leg.get_frame().set_alpha(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions 7.4**\n",
    "\n",
    "   1. Are you surprised by the behavior of the curve associated to the training set?\n",
    "   2. Are you disappointed by the behavior of the curve associated to the validation set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Clustering - The Gaussian Mixture Model\n",
    "\n",
    "We now consider the [Expectation Maximization algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) (EM) in order to estimate mixtures of Gaussians. As presented on [Slides 26 to 41 of the course](http://www.cs.toronto.edu/~lcharlin/courses/80-629/slides_unsupervised.pdf), the idea is simply to associate each observation with a probability of belonging to one or the other of the distributions.\n",
    "\n",
    "Coding the EM algorithm in numpy can be a tedious exercise, so we'll just use the GMM implementation provided by sklearn: [sklearn GMMs](https://scikit-learn.org/0.16/modules/generated/sklearn.mixture.GMM.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we generate some data\n",
    "cluster_std=4\n",
    "X, y = make_blobs(n_samples=1000, centers=2, cluster_std=cluster_std, \n",
    "                  n_features=2, random_state=10)\n",
    "\n",
    "# fit the GMM model\n",
    "nb_components = 2\n",
    "GMM = mixture.GaussianMixture(n_components=nb_components, \n",
    "                              covariance_type='full')\n",
    "GMM.fit(X)\n",
    "\n",
    "# Plot the clustering result, where the color of the points indicate\n",
    "# to which cluster they probabilistically belong to.\n",
    "super_scat_it(X, GMM.predict_proba(X), dim=nb_components, \n",
    "              clusters_center=0, task='EM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions 7.5**\n",
    "\n",
    "1. After a quick glance at the figure above, what do you notice that is different from the k-means algorithm?\n",
    "2. What would have happened if we had set the parameter associated with the variance of the sub-populations (`cluster_std`) to 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1 Choosing the optimal number of clusters for a GMM\n",
    "\n",
    "**Questions 7.6**\n",
    "\n",
    "1. How do we find the right number of clusters?\n",
    "2. Do the train/validation curves behave similarly to those of the k-means algorithm (e.g. in Section 7.2.3)? Does it make sense?\n",
    "2. Could you imagine another way to find the optimal number of mixtures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, _, _ = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "loss_train, loss_valid = [], []\n",
    "\n",
    "for k in np.arange(max_cluster)+1:\n",
    "    GMM = mixture.GaussianMixture(n_components=k, covariance_type='full')\n",
    "    GMM.fit(X_train)\n",
    "    \n",
    "    loss_train.append(GMM.score(X_train))\n",
    "    loss_valid.append(GMM.score(X_valid))\n",
    "            \n",
    "plt.plot(np.arange(max_cluster)+1, loss_train, label='Train')\n",
    "plt.plot(np.arange(max_cluster)+1, loss_valid, label='Validation')\n",
    "\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Log-Likelihood (Maximize)')\n",
    "\n",
    "# Ghosting the legend\n",
    "leg = plt.gca().legend(loc='center left', bbox_to_anchor=(1, .85))\n",
    "leg.get_frame().set_alpha(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Dimensionality Reduction: Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1 Model\n",
    "\n",
    "Autoencoders (AE) are a class of neural networks that allow unsupervised learning of the latent characteristics of the data being studied (see capsule 4 of week 7 or [Chapter 14](http://www.deeplearningbook.org/contents/autoencoders.html) of the [Deep Learning book](http://www.deeplearningbook.org/)). To do this, the AE will attempt to predict, or copy, the input observations using (multiple) transformations (hidden layer). In its simplest form, the architecture of an AE can be summarized in the diagram below.\n",
    "\n",
    "![title](./Images/AE.png)\n",
    "\n",
    "Looking more closely, the AE consists of an encoder, the function $h(\\cdot)$ defined by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    h(\\mathbf{x}) = \\frac{1}{1+ \\exp(-\\mathbf{W} \\mathbf{x})}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This function takes as input the observations and will consist of recoding it as a hidden layer so as to reduce their size (fewer neurons). Afterwards, an encoder defined by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    f(h(\\mathbf{x})) = \\mathbf{W}^\\top h(\\mathbf{x})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "will attempt <i>to reconstruct </i> the input observations from the hidden layer. In this sense, the AE tries to estimate the observations used as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.3 Data simulation\n",
    "\n",
    "Let's first simulate more complex Gaussian mixtures (look at the number of clusters and the dimension of the data) in order to understand the behaviour of the AEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_obs = 10**4  # Number of observations\n",
    "k = 3           # Number of groups\n",
    "std = 0.01      # Standard deviation for each blob\n",
    "dim = 5         # Data dimensions\n",
    "seed = 10       # Seed to control the data generation\n",
    "\n",
    "X, y = make_blobs(n_samples=nb_obs, centers=k, cluster_std=std, \n",
    "                  n_features=dim, random_state=10)   # Data generation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!! Remark !!** \n",
    "\n",
    "There's no specific class for autoencoders in `sklearn`, but since AEs are a type of feed-forward network we can simply reuse the `MLP` classes (e.g., `MLPRegressor`) to build an AE.\n",
    "\n",
    "Note that there exists un library called `scikit-neuralnetwork` which offers [AE models](https://scikit-neuralnetwork.readthedocs.io/en/latest/guide_sklearn.html#unsupervised-pre-training) directly using an sklearn-like interface. You could of course also use PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 12.56434415\n",
      "Validation score: 0.612601\n",
      "Iteration 2, loss = 5.14832218\n",
      "Validation score: 0.817819\n",
      "Iteration 3, loss = 2.60939457\n",
      "Validation score: 0.903460\n",
      "Iteration 4, loss = 1.37251663\n",
      "Validation score: 0.950421\n",
      "Iteration 5, loss = 0.69240844\n",
      "Validation score: 0.975491\n",
      "Iteration 6, loss = 0.33480555\n",
      "Validation score: 0.988312\n",
      "Iteration 7, loss = 0.15612936\n",
      "Validation score: 0.994740\n",
      "Iteration 8, loss = 0.06906778\n",
      "Validation score: 0.997731\n",
      "Iteration 9, loss = 0.02909374\n",
      "Validation score: 0.999088\n",
      "Iteration 10, loss = 0.01164580\n",
      "Validation score: 0.999651\n",
      "Iteration 11, loss = 0.00447650\n",
      "Validation score: 0.999873\n",
      "Iteration 12, loss = 0.00173876\n",
      "Validation score: 0.999956\n",
      "Iteration 13, loss = 0.00075062\n",
      "Validation score: 0.999985\n",
      "Iteration 14, loss = 0.00042251\n",
      "Validation score: 0.999994\n",
      "Iteration 15, loss = 0.00032012\n",
      "Validation score: 0.999996\n",
      "Iteration 16, loss = 0.00029073\n",
      "Validation score: 0.999997\n",
      "Iteration 17, loss = 0.00028294\n",
      "Validation score: 0.999997\n",
      "Iteration 18, loss = 0.00028103\n",
      "Validation score: 0.999997\n",
      "Iteration 19, loss = 0.00028056\n",
      "Validation score: 0.999997\n",
      "Iteration 20, loss = 0.00028050\n",
      "Validation score: 0.999997\n",
      "Iteration 21, loss = 0.00028051\n",
      "Validation score: 0.999997\n",
      "Iteration 22, loss = 0.00028051\n",
      "Validation score: 0.999997\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='logistic', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "             hidden_layer_sizes=2, learning_rate='constant',\n",
       "             learning_rate_init=0.1, max_fun=15000, max_iter=200, momentum=0.9,\n",
       "             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "             random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "             validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings # to remove some sklearn warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "learning_rate = 1e-1\n",
    "aenn = MLPRegressor(hidden_layer_sizes=(2), \n",
    "             activation='logistic', \n",
    "             solver='adam', \n",
    "             alpha=0.0001, \n",
    "             batch_size='auto', \n",
    "             early_stopping=True,\n",
    "             learning_rate_init=learning_rate,\n",
    "             verbose=True)\n",
    "\n",
    "# Since we use early_stopping builtin into the MLPRegressor,\n",
    "# we use both train and validation as our training set.\n",
    "aenn.fit(np.vstack((X_train, X_valid)), np.vstack((X_train, X_valid)))\n",
    "\n",
    "# If we had wanted to use our original validation dataset\n",
    "# we could have used the partial_fit() function\n",
    "# For example,\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "#max_epochs = 100 \n",
    "#mse_train, mse_valid = [], []\n",
    "#for i in range(max_epochs):\n",
    "#    aenn.partial_fit(X_train, X_train)\n",
    "#    train_pred = aenn.predict(X_train)\n",
    "#    mse_train.append(mean_squared_error(X_train, train_pred))\n",
    "#\n",
    "#    valid_pred = aenn.predict(X_valid)\n",
    "#    mse_valid.append(mean_squared_error(X_valid, valid_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7.7**\n",
    "\n",
    "Why should the dimension of the hidden layer be smaller than the dimension of the input layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.5 A representation study\n",
    "\n",
    "Now that the AE is trained, we can look at the latent representation given by the hidden layer. Since we want to visualize the hidden states, we can simply compare the two by two representations from a small amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_sub = 500 # a small amount of data\n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "X_sub = X_train[0:n_sub]\n",
    "y_sub = y_train[0:n_sub]\n",
    "\n",
    "# Get the hidden representations\n",
    "hiddens = expit(np.dot(X_sub, aenn.coefs_[0]) + aenn.intercepts_[0])\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure()\n",
    "hiddens += np.random.randn(n_sub,2)*0.01 # add a bit of noise for vizualization purposes\n",
    "plt.scatter(hiddens[:, 1], hiddens[:, 0], c=y_sub)\n",
    "plt.xlabel('First hidden dimension')\n",
    "plt.ylabel('Second hidden dimension')\n",
    "plt.title('The hidden dimensions of %d %d-dimensional points' % (X_sub.shape))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions 7.8**\n",
    "\n",
    "1. Does the plot behave as you expected? How so?\n",
    "2. Run the same experiment again without changing the hyperparameters. What do you notice? Does the latent representation seems to have changed?\n",
    "3. Would the plot have been different if we had simulated the data from 2 clusters instead of 4? Try it! \n",
    "4. How could you use the information represented in the hidden layers?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
