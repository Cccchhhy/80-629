{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A tiny neural network classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to classify the examples, we will use the following simple neural network:\n",
    "\n",
    "<img src=\"images/nn.png\" width=\"700\">\n",
    "\n",
    "where $\\sigma$ is the sigmoid function defined as:\n",
    "\n",
    "$$\n",
    "    \\sigma(x) = \\frac{1}{1+ e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Assume that the parameters of the neural network are as follows: \n",
    "\n",
    "\\begin{aligned}\n",
    "& w_1 = -5 & w_2 = 10 && w_3 = 5 \\\\\n",
    "& w_4 = -10 & w_5 = 20 && w_6 = 20 \\\\\n",
    "& b_1 = 25 & b_2 = 40 && b_3 = -30 \n",
    "\\end{aligned}\n",
    "\n",
    "What would be the predicted label for the following data points:\n",
    "\n",
    " | x1 | x2 | o | label |\n",
    " |-------|-------|-----|-------|\n",
    " | 4     | -4    |     |       |\n",
    " |-4     | 4     |     |       |\n",
    " | -4    | -4    |     |       |\n",
    " | 4     | 4     |     |       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following piece of code to evaluate the output of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x): \n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def nn1(x1, x2, w1, w2, w3, w4, w5, w6, b1, b2, b3):\n",
    "    h1 = sigmoid(w1*x1 + w3*x2 + b1)\n",
    "    h2 = sigmoid(w2*x1 + w4*x2 + b2)\n",
    "    o = sigmoid(w5*h1 + w6*h2 + b3)\n",
    "    return o\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding good parameters for our network\n",
    "\n",
    "Our ultimate goal is to use this network as a classifier. Let's first load the data that we want to classify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import load_data, plot_boundaries, plot_data\n",
    "X_train, y_train, X_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plot the data using the helper function `plot_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the positive and negative examples can not be separated by a linear classification. Our goal for the rest of this session is to learn a neural network which can separate these positive and negative examples. \n",
    "\n",
    "What do we mean by *learning the parameters*? Remember that our neural network has 9 parameters ($w_1, \\ldots, w_6, b_1, b_2, b_3$). Every different assignment of values to these parameters leads to a different classifier. We want to find the one which matches our data the best. \n",
    "\n",
    "Let's see how different choices of parameters changes the classifier. For a given set of parameter, the function `plot_boundaries` shows the regions of positive prediction (colored blue) and negative prediction (colored red):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 1; w2 = 1; w3 = 1; w4 = 1; w5 = 1; w6 = 1\n",
    "b1 = 0; b2 = 0; b3 = -1\n",
    "plot_boundaries(w1, w2, w3, w4, w5, w6, b1, b2, b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's project the plot of data on these decision boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundaries(w1, w2, w3, w4, w5, w6, b1, b2, b3)\n",
    "plot_data(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the classifier obtained using the above set of parameters does not match our data. \n",
    "\n",
    "### Exercise\n",
    "Try the alternatives below and see which one is a better match for our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = -1; w2 = -1; w3 = -1; w4 = -1; w5 = 4; w6 = -3\n",
    "b1 = -4; b2 = 4; b3 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 1; w2 = -1; w3 = -1; w4 = -1; w5 = -4; w6 = 3\n",
    "b1 = 4; b2 = -4; b3 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = -1; w2 = 2; w3 = 1; w4 = -2; w5 = 4; w6 = 4\n",
    "b1 = 5; b2 = 8; b3 = -6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we need a better way than trial and error to find the best parameters. The way that we do this is by *minimizing a loss function*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *loss function* evaluates how much the predictions of our classifier are different from the actual labels. The loss function that we will use for our network is the *binary cross-entropy* loss. Let's represent our training data by the set $\\{(X_1, y_1), \\ldots, (X_n , y_n)\\}$ and our neural network function by $f$. Then the binary cross-entropy loss function will be defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\ell = \\sum_{i=1}^n -y_i \\log f(X_i) - (1-y_i) log(1-f(X_i))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Let's see what this loss function means using a tiny example. Assume that our training data consists of only four examples, and the values of $X, f(X), y$ of those four examples are as follows:\n",
    "\n",
    "|X|f(X)|y|\n",
    "|:---|:---|:---|\n",
    "|(5.4, 1.6)|1|1|\n",
    "|(1.4, -0.5)|0.3679|1|\n",
    "|(3.5, -3)|0.8647|0|\n",
    "|(-3.5, 1.1)|0|0|\n",
    "\n",
    "Calculate the loss function using the equation above. You can calculate the *log* using this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.log(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to remember that the loss function $l$ is a function of network parameters, since it is defined in terms of the network output. We can write the loss function as:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\ell(\\mathbf{w}, \\mathbf{b}) = \\sum_{i=1}^n -y_i \\log f(X_i, \\mathbf{w}, \\mathbf{b}) - (1-y_i) log(1-f(X_i, \\mathbf{w}, \\mathbf{b}))\n",
    "\\end{equation}\n",
    "\n",
    "In principle, we want to find the set of parameters $\\mathbf{w}, \\mathbf{b}$ for which $\\ell(\\mathbf{w}, \\mathbf{b})$ has the smallest value. We will use *gradient descent* to find these values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimization by gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below shows the function $f(x_1, x_2) = x_1^2 + x_2^2$:\n",
    "\n",
    "<img src=\"images/descent.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Point A on the plot has coordinates $(1, 1, 3)$. The blue vector AB shows the direction $(-1, -1)$, and the green vector AC shows the direction $(0, -1)$. \n",
    "Assume that we are at initial point $(1, 1)$ and we want to move in a direction that minimizes the function $f$. Which of these two directions moves faster towards the minimum: $(-1, -1)$ or $(0, -1)$?\n",
    "\n",
    "### Exercise\n",
    "Calculate the gradient of function $f$ in the point $(1, 1)$. How is this gradient related to the fastest path to the minimum (i.e. the steepest descent)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the neural network\n",
    "\n",
    "We now understand the theory of training neural networks. But how do we do this in practice? In this section we will use *scikit-learn* to train our tiny network. Let's first define the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(2,), \n",
    "                    activation='logistic', \n",
    "                    solver='lbfgs',\n",
    "                    random_state=0,\n",
    "                    max_iter=500,\n",
    "                    tol=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument `hidden_layer_sizes=(2,)` states that we only have one hidden layer with two neurons, and the argument `activation='logistic'` shows that we use the sigmoid activation function (Let's ignore the other arguments for now). \n",
    "\n",
    "We will now train the network using our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the network is trained, use the helper function `tiny_net_parameters` to get the parameters of the trained network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import tiny_net_parameters\n",
    "w1, w2, w3, w4, w5, w6, b1, b2, b3 = tiny_net_parameters(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_boundaries(w1, w2, w3, w4, w5, w6, b1, b2, b3)\n",
    "plot_data(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learned classifier does a good job at predicting labels both for the training examples and unseen examples (test data). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground exercises\n",
    "\n",
    "We will now investigate a few properties of neural networks using [tensorflow playground](https://playground.tensorflow.org/). Take a few minutes to make yourself familiar with the playground:\n",
    "\n",
    "- Change the number of hidden layers to one\n",
    "- Change the data distribution to *exclusive OR*\n",
    "- Push the *run* button and see how the network is trained\n",
    "- Stop training after epoch 500\n",
    "- Hover over the neurons in the hidden layer and see the vizualization of their outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate\n",
    "\n",
    "Open [this](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=3&regularizationRate=0&noise=35&networkShape=1&seed=0.68448&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) example on tensorflow playground. \n",
    "\n",
    "- Push the *run* button and see the learning process for 500 epochs. What do you observe?\n",
    "- Stop training and press the *restart* button. Change the learning rate from 3 to 0.1, and press the *run* button again. What is different from the previous run?\n",
    "- Try these steps using three learning rates: 0.3, 0.03, and 0.003:\n",
    " + Press the *reset* button\n",
    " + Change the learning rate\n",
    " + Press the *step* button (located at the right of *run* button) a few times, and observe how the training/test loss changes in each step. \n",
    " \n",
    "Which of those three rates would you use? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Open [this](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=50&networkShape=4,4&seed=0.64895&showTestData=false&discretize=false&percTrainData=10&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) example on palyground. \n",
    "\n",
    "Let's first observe a few things about this example. Check the box titled *Show test data*. Uncheck the box again. As you can see, the data is noisy and the number of training examples is small. This is a situation prone to overfitting. \n",
    "- Press the *run* button and let the training proceed for near 500 epochs, then pause the training. \n",
    "- What do you think about the decision boundary of the classifier? \n",
    "- What causes the difference between the training error and test error? (Check the *Show test data* box again)\n",
    "- Write down the test error\n",
    "\n",
    "We will now see how we can avoid overfitting using $L_2$ regularization. \n",
    "- Press the *restart* button\n",
    "- Change *regularization* from *None* to *L2*\n",
    "- Change *Regularization rate* from 0 to 0.3\n",
    "- Press the *run* button and run the model for 500 epochs\n",
    "- What is different from the previous setting? \n",
    "- Write down the test error\n",
    "\n",
    "Just like learning rate, different regularization rates will affect the classifier performance. Try these steps with regularization rates 0.03 and 0.003:\n",
    "- Press the *restart* button\n",
    "- Change *Regularization rate*\n",
    "- Press the *run* button and run the model for 500 epochs\n",
    "- Write down the test error\n",
    "\n",
    "Which of these regularization rates would you use?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
