{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH80629\n",
    "\n",
    "# Semaine \\#5 - Réseaux de neurones - Exercices\n",
    "\n",
    "Ce tutoriel explore les réseaux de neurones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-28 13:37:30--  https://raw.githubusercontent.com/lcharlin/80-629/master/week5-NeuralNetworks/utils.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13166 (13K) [text/plain]\n",
      "Saving to: ‘utils.py.4’\n",
      "\n",
      "utils.py.4          100%[===================>]  12.86K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-09-28 13:37:30 (18.8 MB/s) - ‘utils.py.4’ saved [13166/13166]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pour obtenir le fichier utils.py\n",
    "!wget -O https://raw.githubusercontent.com/lcharlin/80-629/master/week5-NeuralNetworks/utils.py utilities.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un tout petit réseau pour se faire la main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons utiliser ce réseau de neurones pour classifier des données:\n",
    "\n",
    "<img src=\"images/nn.png\" width=\"700\">\n",
    "\n",
    "avec $\\sigma$ la fonction sigmoïde:\n",
    "\n",
    "$$\n",
    "    \\sigma(x) = \\frac{1}{1+ e^{-x}}\n",
    "$$\n",
    "\n",
    "Pour l'instant, nous n'allons pas entraîner le modèle. Nous allons simplement calculer ses sorties avec des poids (paramètres) fixes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Utilisez ces paramètres du réseau de neurones: \n",
    "\n",
    "\\begin{aligned}\n",
    "& w_1 = -5 & w_2 = 10 && w_3 = 5 \\\\\n",
    "& w_4 = -10 & w_5 = 20 && w_6 = 20 \\\\\n",
    "& b_1 = 25 & b_2 = 40 && b_3 = -30 \n",
    "\\end{aligned}\n",
    "\n",
    "Pour obtenir une prédiction (o et cible) pour les données (x) suivantes :\n",
    "\n",
    "\n",
    " | x1 | x2 | o | cible |\n",
    " |-------|-------|-----|-------|\n",
    " | 4     | -4    |     |       |\n",
    " |-4     | 4     |     |       |\n",
    " | -4    | -4    |     |       |\n",
    " | 4     | 4     |     |       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez utiliser ces fonctions pour évaluer les sorties du réseau :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def nn1(x1, x2, w1, w2, w3, w4, w5, w6, b1, b2, b3):\n",
    "    h1 = sigmoid(w1*x1 + w3*x2 + b1)\n",
    "    h2 = sigmoid(w2*x1 + w4*x2 + b2)\n",
    "    o = sigmoid(w5*h1 + w6*h2 + b3)\n",
    "    return o\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "nn1() missing 10 required positional arguments: 'x2', 'w1', 'w2', 'w3', 'w4', 'w5', 'w6', 'b1', 'b2', and 'b3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1915c3f1df02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# La règle de décision classifie les valeurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: nn1() missing 10 required positional arguments: 'x2', 'w1', 'w2', 'w3', 'w4', 'w5', 'w6', 'b1', 'b2', and 'b3'"
     ]
    }
   ],
   "source": [
    "w1 = ...; w2 = ...; w3 = ...; w4 = ...; w5 = ...; w6 = ...\n",
    "b1 = ...; b2 = ...; b3 = ...\n",
    "\n",
    "for (x1, x2) in [(4, -4), (-4, 4), (-4, -4), (4, 4)]:\n",
    "    o = nn1(...)\n",
    "    \n",
    "    # La règle de décision classifie les valeurs \n",
    "    #  prédites plus grandes que 0.5 comme venant de la classee 1\n",
    "    #  et les autres (<=0.5) comme venant de la classe 0.\n",
    "    if o > 0.5:\n",
    "        label = 1 \n",
    "    else:\n",
    "        label = 0\n",
    "    print('x1:%d  x2:%d  output:%.2f, label:%d'%(x1, x2, o, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trouvons de bons paramètres à notre réseau\n",
    "\n",
    "Évidemment en pratique nous aimerions entraîner les poids de notre réseau à partir d'un ensemble d'entraînement. \n",
    "\n",
    "Pour ces exemples, nous nous concentrons sur la classification binaire. \n",
    "\n",
    "---\n",
    "\n",
    "Comme d'habitude, on commence par charger un jeu de données (de classification):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utilities import load_data, plot_boundaries, plot_data # on a écrit quelques fonctions \n",
    "X_train, y_train, X_test, y_test = load_data()          # pour obtenir des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données sont en deux dimensions et nous pouvons les visualiser avec la fonction `plot_data` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces données ne sont pas linéairement séparables (à tout le moins, il faudrait combiner la décision de deux modèles linéaires). \n",
    "\n",
    "Le but du reste des exercices est d'apprendre des paramètres du réseau de neurones qui nous permettront de correctement discriminer les exemples des deux classes. \n",
    "\n",
    "Rappel: Que veut-on dire par *apprendre des paramètres*? Notre réseau de neurones à 9 paramètres incluant 3 interceptes ($w_1, \\ldots, w_6, b_1, b_2, b_3$). Chaque ensemble de valeurs des poids mène à une classificateur différent. Nous voulons donc trouver les valeurs qui nous permettront de classifier le plus justement possible nos données.\n",
    "\n",
    "Débutons par explorer comme les différents paramètres affectent la classification. Pour un ensemble de paramètres, la fonction `plot_boundaries` permet de visualiser les frontières de décisions ainsi que les prédictions du classificateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 1; w2 = 1; w3 = 1; w4 = 1; w5 = 1; w6 = 1\n",
    "b1 = 0; b2 = 0; b3 = -1\n",
    "plot_boundaries(w1, w2, w3, w4, w5, w6, b1, b2, b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut y surimposer les données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundaries(w1, w2, w3, w4, w5, w6, b1, b2, b3)\n",
    "plot_data(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Évidemment, le classificateur n'est pas entraîné sur nos données et donc il ne les classifie pas bien. (*Rappel&nbsp;:* Ce classificateur avec des paramètres fixés à priori a un haut biaise et une petite variance.)\n",
    "\n",
    "### Question 2\n",
    "Essayez les autres ensembles de poids ci-dessous et trouvez celui qui fonctionnerait le mieux sur nos données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = -1; w2 = -1; w3 = -1; w4 = -1; w5 = 4; w6 = -3\n",
    "b1 = -4; b2 = 4; b3 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 1; w2 = -1; w3 = -1; w4 = -1; w5 = -4; w6 = 3\n",
    "b1 = 4; b2 = -4; b3 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = -1; w2 = 2; w3 = 1; w4 = -2; w5 = 4; w6 = 4\n",
    "b1 = 5; b2 = 8; b3 = -6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Évidemment, nous avons besoin d'une méthode systématique pour trouver les paramètres. Comme c'était le cas pour les modèles linéaires, nous allons apprendre les poids en *minimisant une fonction de perte (loss)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction de perte (loss function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une *fonction de perte* évalue la différence entre les prédictions de notre classificateur et les véritables cibles (en d'autres mots on peut aussi la voir comme une fonction qui évalue la qualité de notre modèle). \n",
    "\n",
    "La fonction de perte que nous utilisons pour notre réseau est l'entropie croisée binaire (*binary cross-entropy*). Si nous représentons notre ensemble d'entraînement avec l'ensemble $\\{(X_1, y_1), \\ldots, (X_n , y_n)\\}$ et notre réseau de neurones par la fonction $f$, alors l'entropie croisée binaire est :\n",
    "\n",
    "\\begin{equation}\n",
    "    \\ell = \\sum_{i=1}^n -y_i \\log f(X_i) - (1-y_i) log(1-f(X_i))\n",
    "\\end{equation}\n",
    "\n",
    "Il est important de noter que cette fonction d'erreur est calculée avant la frontière de décision (donc sur les valeurs de `o`). Intuitivement, ça permet de plus facilement apprendre de bons poids puisque sinon les gradients seraient souvent de 0. \n",
    "\n",
    "L'entropie croisée binaire est reliée à la distribution de Bernoulli (maximiser la vraisemblance sous une Bernoulli likelihood est équivalent à minimiser l'entropie croisée). **C'est la fonction de perte standard à utiliser pour les tâches de classification binaire. Pour la classification multiclasse, on utilise l'entropie croisée.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Essayons d'obtenir quelques intuitions sur cette fonction de perte. \n",
    "\n",
    "Imaginons que notre ensemble d'entraînement ne contienne que quatre exemples, et ces valeurs pour $X, f(X), y$ comme suit:\n",
    "\n",
    "|X|f(X)|y|\n",
    "|:---|:---|:---|\n",
    "|(5.4, 1.6)|1|1|\n",
    "|(1.4, -0.5)|0.3679|1|\n",
    "|(3.5, -3)|0.8647|0|\n",
    "|(-3.5, 1.1)|0|0|\n",
    "\n",
    "Calculez la fonction de perte en utilisant l'équation plus haut. Vous pouvez calculer le logarithme à l'aide de la fonction suivante de `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.log(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 0\n",
    "for fx, y in [(1, 1), (0.3679, 1), (0.8647, 0), (0, 0)]:\n",
    "    ... \n",
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N'oubliez pas que la fonction de perte $l$ est une fonction des paramètres du réseau puisque la perte est définie en fonction des sorties du réseau. En effet, nous pouvons écrire la fonction de perte comme ça : \n",
    "\n",
    "\\begin{equation}\n",
    "    \\ell(\\mathbf{w}, \\mathbf{b}) = \\sum_{i=1}^n -y_i \\log f(X_i, \\mathbf{w}, \\mathbf{b}) - (1-y_i) log(1-f(X_i, \\mathbf{w}, \\mathbf{b}))\n",
    "\\end{equation}\n",
    "\n",
    "En principe, nous voulons trouver l'ensemble des paramètres $\\mathbf{w}, \\mathbf{b}$ pour lesquels $\\ell(\\mathbf{w}, \\mathbf{b})$ a la plus petite valeur. Nous utiliserons la *descente du gradient* pour trouver ces valeurs.\n",
    "\n",
    "---  \n",
    "\n",
    "*Remarque:* Pour entraîner les modèles, il nous suffit de calculer la dérivée de la fonction de perte en fonction des paramètres. Dans le cas de la régression linéaire, une fois que nous avions posé que la dérivée doit être égale à 0,   nous avions pu isoler w pour obtenir une expression analytique $w_{ols} = (X X^\\top)^{-1} X^\\top y)$.\n",
    "\n",
    "Dans le cas d'un réseau de neurones, nous ne pouvons pas isoler les w. Par contre, souvenons-nous que la dérivée nous indique la pente de la fonction. Nous pouvons donc suivre cette pente pour tenter de trouver les paramètres qui la minimisent. En pratique, ça nous donne une procédure itérative. À chaque itération, nous calculons la dérivée, nous la « suivons » et nous recommençons.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimisation par la descente du gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La figure ci-dessous nous montre la fonction $f(x_1, x_2) = x_1^2 + x_2^2$ :\n",
    "\n",
    "<img src=\"images/descent.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Le point A sur la figure est aux coordonnées $(1, 1, 3)$. Le vecteur bleu AB pointe dans la direction $(-1, -1)$ et le vecteur vert AC pointe dans la direction $(0, -1)$. \n",
    "\n",
    "Imaginez que vous êtes au point $(1, 1)$ et que vous vouliez vous déplacer dans une direction qui minimise la fonction $f$. \n",
    "\n",
    "Avec laquelle de ces deux directions atteindrez-vous le minimum en premier : $(-1, -1)$ or $(0, -1)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Calculez le gradient de la fonction $f$ au point $(1, 1)$. Comment ce gradient est-il relié à la direction directe pour aller au minimum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraîner le réseau de neurones\n",
    "\n",
    "Maintenant que nous avons une meilleure intuition sur la descente de gradient, on peut se demander comment l'implémenter en pratique. \n",
    "\n",
    "Nous allons utiliser la librairie *scikit-learn* library pour entraîner notre petit réseau. On commence par définir le réseau&nbsp;:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(2,),\n",
    "                    activation='logistic', \n",
    "                    solver='lbfgs',\n",
    "                    random_state=1234,\n",
    "                    max_iter=500,\n",
    "                    tol=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `hidden_layer_sizes=(2,)` indique que nous utilisons une seule couche cachée avec deux neurones\n",
    "- `activation='logistic'` indique que nous utilisons la fonction d'activation sigmoïde \n",
    "(on peut ignorer les autres arguments pour l'instant). \n",
    "\n",
    "Avec la fonction `fit()` on peut entraîner le réseau sur nos données d'entraînement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois que notre réseau est entraîné, on peut appeler la fonction `tiny_net_parameters` pour obtenir les paramètres du réseau entraîné (`tiny_net_parameters` utilise simplement les valeurs `clf.coefs_` et `clf.intercepts_` de `scikit-learn`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import tiny_net_parameters\n",
    "w1, w2, w3, w4, w5, w6, b1, b2, b3 = tiny_net_parameters(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_boundaries(w1, w2, w3, w4, w5, w6, b1, b2, b3)\n",
    "plot_data(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le classificateur appris classifie bien les exemples des ensembles d'apprentissages et de tests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "En plus des frontières de décisions dans l'espace original des données, nous pouvons aussi visualiser comment les données sont transformées à travers le réseau de neurones. Nous utilisons le fait que la couche cachée utilise deux neurones et donc nous pouvons visualiser sa sortie en deux dimensions. \n",
    "\n",
    "(pour une meilleure visibilité, les points de la classe jaune seront maintenant en bleu.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import plot_data_transformations\n",
    "plot_data_transformations(X_train, y_train, w1, w2, w3, w4, w5, w6, b1, b2, b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notons que les données sont progressivement transformées pour finalement être linéairement séparables à la sortie du réseau. Dans ce cas, on imagine que l'erreur d'entraînement est de zéro. \n",
    "\n",
    "On peut obtenir le même rendu avec les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_transformations(X_test, y_test, w1, w2, w3, w4, w5, w6, b1, b2, b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats sont similaires et les données transformées sont linéairement séparables. On voit qu'elles ne sont pas aussi distancées dans la transformation finale (figure de droite)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question sur Tensorflow Playground\n",
    "\n",
    "Nous allons maintenant explorer les réseaux de neurones avec un outil très visuel appelé [tensorflow playground](https://playground.tensorflow.org/). Le but est de mieux comprendre l'effet des différents hyperparamètres sur l'entraînement et la généralisation des réseaux.\n",
    "\n",
    "Vous pouvez d'abord vous familiariser avec son interface. Ensuite, essayez de reproduire ces instructions&nbsp;:\n",
    "\n",
    "- Utilisez une seule couche cachée\n",
    "- Changez la distribution des données (*data distribution*) pour utiliser le *exclusive OR* (ou XOR)\n",
    "- Lancez l'entraînement en appuyant sur *run*. Vous allez voir que le réseau va commencer à apprendre\n",
    "- Arrêtez l'apprentissage après 500 epoch (une epoch consiste à procéder à une descente de gradient sur tous les exemples. Quand on utilise SGD, c'est donc plus qu'une itération de la descente de gradient.)\n",
    "- En positionnant votre curseur sur les différents neurones, vous obtiendrez leur frontière de classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taux d'apprentissage (Learning rate)\n",
    "\n",
    "Ouvrez [cet exemple](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=3&regularizationRate=0&noise=35&networkShape=1&seed=0.68448&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) sur tensorflow. \n",
    "\n",
    "- Lancez l'entraînement (bouton *run*) pour environ 500 epochs. Qu'observez-vous? \n",
    "- Réinitialisez le réseau (flèche *restart*). Changez le taux d'apprentissage de 3 à 0.1 et appuyez sur *run*. Laissez le réseau d'entraîner pour 500 epochs. Qu'observez-vous?   \n",
    "\n",
    "- Refaites l'entraînement avec les trois taux d'apprentissage suivant&nbsp; 0.3, 0.03 et 0.003\n",
    " + Appuyez sur le bouton *reset*\n",
    " + Changez le taux d'apprentissage\n",
    " + Appuyez sur le bouton *step* (à droite du bouton *run*) quelques fois et observez comment les erreurs d'entraînement et de test changent à chaque étape. \n",
    "\n",
    "Selon vos observations, quel taux d'apprentissage devriez-vous utiliser?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Régularisation\n",
    "\n",
    "Utilisez [cet exemple](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=50&networkShape=4,4&seed=0.64895&showTestData=false&discretize=false&percTrainData=10&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false).\n",
    "\n",
    "Avant de lancer l'entraînement, prenez un moment pour inspecter l'ensemble de test (à droite de l'écran, il y a l'option *Show test data*). Comme vous pouvez le voir, les données sont bruitées, de plus le nombre de données d'entraînement est petit. Cette situation est parfaite pour le surentraînement. (Vous pouvez enlever l'option *Show test data*.)\n",
    "\n",
    "- Appuyez sur le bouton *run* et laissez le réseau s'entraîner pour 500 epochs avant d'arrêter l'entraînement.\n",
    "- Que pensez-vous de la frontière de décision trouvée par le réseau? \n",
    "- Quelle est la cause de la différence entre les courbes d'entraînement et de test? (Vous pouvez réutiliser l'option *Show test data*)\n",
    "- Prenez note de l'erreur de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explorons comment limiter le surentraînement avec une régularisation $L_2$. \n",
    "- Appuyez sur *restart*\n",
    "- Changez la *regularization* de *None* à *L2*\n",
    "- Changez *Regularization rate* de 0 à 0.3\n",
    "- Lancez l'entraînement (appuyez sur *run*) pour environ 500 epochs\n",
    "- Qu'observez-vous par rapport à l'expérience précédente?\n",
    "- Notez l'erreur de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme le taux d'apprentissage, différents taux de régularisation vont affecter la performance d'entraînement. \n",
    "\n",
    "Relancez l'entraînement avec ces taux de régularisation 0.03 et 0.003:\n",
    "- Appuyez sur *restart*\n",
    "- Changez le *Regularization rate*\n",
    "- Appuyez sur *run* et laissez le modèle s'entraîner pour environ 500 epochs\n",
    "- Notez l'erreur de test\n",
    "\n",
    "Quel taux de régularisation utiliseriez-vous? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
