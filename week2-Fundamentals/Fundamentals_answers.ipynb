{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '80-629'...\n",
      "remote: Enumerating objects: 61, done.\u001b[K\n",
      "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
      "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
      "Receiving objects:  30% (34/113), 61.07 MiB | 5.19 MiB/s\r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "!rm -rf 80-629\n",
    "!git clone https://github.com/lcharlin/80-629/\n",
    "import sys\n",
    "sys.path += ['80-629/week2-Fundamentals/']\n",
    "\n",
    "# We import several home functions in order to create graphics\n",
    "from utilities import scatter_plot, plot_polynomial_curves, plot_optimal_curve, train_poly_and_see, MSE\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week \\#2 - Machine Learning Fundamentals - Exercices\n",
    "\n",
    "This tutorial will focus on three important aspects of machine learning (ML), namely the capacity of models, the notions of bias and variance of an estimator as well as a brief introduction to cross-validation. The goal is to develop basic intuition of these concepts through a series of short exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Model capacity\n",
    "\n",
    "Informally, the capacity of a model can be defined as the number of functions a model can fit. Lower-capacity models would be able to perfectly fit (i.e., obtain 0 train error) less functions then higher-capacity models. \n",
    "\n",
    "Higher-capacity models are generally more prone to **overfitting**. Overfitting occures when the gap between the test and training error is large, or in other words when models memorize properties of the training set that are not usefull for (i.e. do not generalize to) performing predictions on a test set.\n",
    "\n",
    "Intuitively, when two models fit the training data equally well, usually the model with less capacity will generalize better, i.e. have lower test error. Thus, as rule of thumb we perfom similar decision rules over more complex ones. (extra: this is a good illustration of https://en.wikipedia.org/wiki/Occam%27s_razor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Data generation\n",
    "\n",
    "Let's first simulate some data! In this section, every observation $y$ is generated according to the following model:\n",
    "\n",
    "$$ y = x \\cos(x / \\gamma) + \\epsilon$$\n",
    "\n",
    "where $y \\in \\mathbb{R}$ is the ouptut, $x \\in \\mathbb{R}$ are the features, $\\gamma$ is the period of the cosine function and $\\epsilon$ is the random noise such as $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ where $\\sigma$ is defined by YOU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_simulation(sample_size, scale, period, variance):\n",
    "    \n",
    "    x = np.random.uniform(-scale, scale, sample_size)\n",
    "    x.sort()\n",
    "    noise = np.random.normal(0, variance, sample_size)\n",
    "    y = x * np.cos(x / period) + noise\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever it is possible, it is always a good idea idea to visualize the data (in order to get some intuition about it). \n",
    "\n",
    "**Question**: Play with the parameters (*variance*, *scale* and *period*) and see how they affect the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_size = 300\n",
    "variance = 10   # Variance of the Gaussian noise\n",
    "scale = 100   # Range\n",
    "period = 6   # Simulation are based on cosinus function (see data_simulation function)\n",
    "\n",
    "x_train, y_train = data_simulation(int(.7*sample_size), scale, period, variance)\n",
    "x_test, y_test = data_simulation(int(.3*sample_size), scale, period, variance)\n",
    "\n",
    "plt = scatter_plot(x_train, x_test, y_train, y_test)   # The scatter_plot function is in the utilities script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Getting - visual - intuition about models' capacity\n",
    "\n",
    "As seen in class (Slide 29 for example), the higher is the capacity of the model, the better it will fit the training data set (caution though, fitting well the training data does not necesarily lead to good generalization). Here, we use [polynomial regression](https://en.wikipedia.org/wiki/Polynomial_regression) to fit the training set (don't worry, the purpose of the tutorial is not to understand polynomial regression). Note however that the greater is the polynomial degree, the higher is the model capacity. \n",
    "\n",
    "**Questions**: \n",
    "1. Observe how the fitted curve behave with respect to their polynomial degre. \n",
    "2. Would you prefer to fit the data points with polynomial regression of degre 25 or 100?\n",
    "3. Wich of these curves should have the best generalization error?\n",
    "\n",
    "**Answers**:\n",
    "\n",
    "As the degre increase, the associated polynomial curve looks as an polynomial interpolation.\n",
    "Looking at the dataset (first Figure, Section 2.1.1), we can see at first sight a maximum of 15 local minima/maxima. This being said, polynomial regression with degre 100 seems overkill (to much capacity).\n",
    "The MSE on the test set (see below) prove us right: polynomial regression of degre 20 seems to generalize than the polynomial regression of degre 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "degre = [0, 1, 3, 5, 10, 20, 150]   # Maximal polynomial degre of the fitted curve: higher degre == higher capacity\n",
    "\n",
    "plot_polynomial_curves(x_train, x_test, y_train, y_test, degre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Sample size and models' capacity\n",
    "\n",
    "We study the behavior of the polynomial regressors and examine how they perform when the sample size increases (as presented on Slide 32). Specificaly, with study the behavior of the cubic polynomial regression and the optimal polynomial regression (which minimize the MSE on the test set). \n",
    "\n",
    "**Question**: Do the following curves behave as expected?\n",
    "\n",
    "**Answer**: Yes! Without running the experiment, we should expect that:\n",
    "\n",
    "1. The MSE on the test set decrease as the train set gets larger.\n",
    "3. The training and the test curves overlap as the training set gets larger. \n",
    "2. Cubic polynomial regression has a greater MSE (than the optimal model) on both training and test sets given a large enough training set (say $n > 100$ for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = [10, 10**2, 10**3, 10**4, 10**5, 10**6]   # Sample size of the training set that we want to study\n",
    "variance = 20\n",
    "\n",
    "H_train, H_test, optimal_train, optimal_test, optimal_degre \\\n",
    "    = train_poly_and_see(sample_size, scale, period, variance, degre)\n",
    "\n",
    "plot_optimal_curve(optimal_train, optimal_test, H_train, H_test, optimal_degre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Bias and variance of estimators\n",
    "\n",
    "We will now explore some properties of the bias and the variance of well known estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Linear models for regression\n",
    "\n",
    "The polynomial curve fitting problem encountered previously is an instance of a broader class of models - linear models. More specifically, it is a linear regression task, where the goal is to predict a value of one or more continuous target variables given the values of some input variables. Linear models share the property of **being a linear function of the adjustable parameters** (polynomial coefficients in case of polynomial models). This simple form of linear regression models are also linear functions of the input variables. Linear models have been studied in depth by statisticians in the last century and their theory is well understood. \n",
    "\n",
    "In addition we hypothesize that our loss function is the squared error (i.e., ( $\\sum_{i=0}^n (y_i - \\hat{y}_i)^2$ ). We will study some properties of this model and loss function. \n",
    "\n",
    "A linear regression model fitted with a squared error is also known as an ordinary least square (OLS) regression.\n",
    "\n",
    "First, let's simulate some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Data simulation\n",
    "\n",
    "In this section, every observation $y$ is generated according to the following model:\n",
    "\n",
    "$$ y = \\bf{x}^\\top \\bf{w} + \\epsilon$$\n",
    "\n",
    "where $y \\in \\mathbb{R}$ is the ouptut, $\\bf{x}$ is the vector of covariates,  $\\bf{w}$ is the vector of the associated weights and $\\epsilon$ is the random noise such as $\\epsilon \\sim \\mathcal{N}(0,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_simulation(sample_size, w_0, w_1):\n",
    "    \n",
    "    x = np.random.uniform(-1, 10, sample_size)\n",
    "    x.sort()\n",
    "    noise = np.random.normal(0, 1, sample_size)\n",
    "    y = w_0 + w_1 * x + noise\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the above model formulation, we can sample the data and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0, w_1 = 2, 3   # Fix w values\n",
    "sample_size = 500   # Fix the sample size - train\n",
    "\n",
    "X, y = data_simulation(sample_size, w_0, w_1)\n",
    "X = [np.ones(len(y)), X]\n",
    "X = np.asarray(X ).T\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "scatter_plot(X_train[:, 1], X_test [:, 1], y_train, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 The OLS estimators\n",
    "\n",
    "**Questions**: \n",
    "1. Given the expression of the least squares estimators presented on slide 24 of the course, complete the OLS function below to obtain the least squares estimators. As a reminder, these estimators are defined as follows:\n",
    "\n",
    "$$ \\hat{\\bf{w}}^{\\text{OLS}} := (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\bf{y}$$\n",
    "\n",
    "where $\\bf{X}$ is the [design matrix](https://en.wikipedia.org/wiki/Design_matrix#:~:text=In%20statistics%2C%20a%20design%20matrix,specific%20values%20for%20that%20object.) and $\\bf{y}$ is the outputs vector.\n",
    "\n",
    "2. Derive the estimators associated with the previously simulated data.\n",
    "\n",
    "*Remark*: Do not forget to calculate the intercept $\\bf{w}_0$. This being said and according to the above OLS function, simply fill the first column of design matrix with ones as explictly suggest [here](https://en.wikipedia.org/wiki/Design_matrix#:~:text=In%20statistics%2C%20a%20design%20matrix,specific%20values%20for%20that%20object.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLS(X, y):\n",
    "    \n",
    "    A = np.linalg.inv(np.dot(X.T, X))\n",
    "    B = np.dot(X.T, y)\n",
    "    \n",
    "    return np.dot(A, B)\n",
    "\n",
    "w_ols = OLS(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Make predictions on training and test sets and compute the associated MSE. are the results obtained those hoped for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set\n",
    "y_hat_train = np.dot(X_train, w_ols)\n",
    "\n",
    "# Test set\n",
    "y_hat_test = np.dot(X_test, w_ols)\n",
    "\n",
    "print('MSE of the train: ', MSE(y_hat_train, y_train))\n",
    "print('MSE of the test:  ', MSE(y_hat_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2.1 Bias of the OLS estimators\n",
    "\n",
    "**Question**: Calculate the bias of the estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = w_ols - [w_0, w_1]\n",
    "\n",
    "print(\"Bias of w_0: \", bias[0])   # Bias of w_0\n",
    "print(\"Bias of the w_1: \", bias[1])   # Bias of w_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Does it make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Indeed! Since the OLS estimator in the contexrt of a linear regression unbiased:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbb{E}[\\hat{\\bf{w}}^{\\text{OLS}}] \n",
    "    &= \\mathbb{E}[(\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\bf{y}] \\\\\n",
    "    &= (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\mathbb{E}[\\bf{y}] \\\\\n",
    "    &= (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\mathbb{E}[\\bf{X} \\bf{w} + \\bf{\\epsilon}] \\\\\n",
    "    &= (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\{ \\mathbb{E}[\\bf{X} \\bf{w}] + \\mathbb{E}[\\bf{\\epsilon}] \\} \\\\\n",
    "    &= (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\{ \\bf{X} \\bf{w} + \\bf{O} \\} \\\\\n",
    "    &= (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\bf{X} \\bf{w}  \\\\\n",
    "    &= \\bf{w}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How could we reduce the observed bias of the OLS estimators?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: In this specific context, one gentle way would be to simply increase the sample size. Try it! Fix the sample size to $n = 100k$ and observe how the empirical biases behave (run everything from Section 2.2.1 with sample_size fixed to 100k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10**6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2.2 Variance of the OLS estimators\n",
    "\n",
    "The OLS estimators calculated in the last section have no fixed values and are actually [random variables](https://en.wikipedia.org/wiki/Random_variable). It follows that these estimators have variance. This variance can be estimated in several ways. In this tutorial, we propose to estimated the variance of the OLS estimators in two different ways. The firs approach use [Monte Carlo methods](https://en.wikipedia.org/wiki/Monte_Carlo_method#Use_in_mathematics) and the second one simply use [theoretical properties of the variance](https://en.wikipedia.org/wiki/Variance#Properties).\n",
    "\n",
    "\n",
    "#### Monte Carlo methods\n",
    "\n",
    "Don't be intimidated by the name, basic Monte Carlo methods are rather simple! As far as we are concerned, we can compute the variance of the OLS estimators in 4 easy steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Complete the code below according to the commentaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_estimates = 100   # Fix the number of Monte Carlo estimates that you want to run\n",
    "M = np.zeros((mc_estimates, 2))   # Initialize a matrix where you will store the estimators\n",
    "\n",
    "# Step 1: Create a for loop \n",
    "for k in np.arange(mc_estimates):\n",
    "    \n",
    "    # Step 2: Data simulation\n",
    "    x, y = data_simulation( int(.8 * sample_size), w_0, w_1)\n",
    "    \n",
    "    X = [np.ones(len(y)), x]   \n",
    "    X = np.asarray(X).T\n",
    "    \n",
    "    # Step 3: OLS estimates\n",
    "    w_ols = OLS(X, y)\n",
    "    M[k, :] = w_ols   # Store the estimators\n",
    "    \n",
    "# Step 4: Compute the variance (hint: np.var)\n",
    "var = np.var(M, axis=0)\n",
    "print(\"MC estimate of the variance of the w_0 estimate: \", var[0])   # Variance of w_0\n",
    "print(\"MC estimate of the variance of the w_1 estimate: \",var[1])   # Variance of w_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analytical approach\n",
    "\n",
    "If we cannot compute the MC estimators, we can sometimes derive the analytical expression of the variance of the estimators. Indeed, the variance of the OLS is:\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{Var}(\\hat{\\bf{w}}^{\\text{OLS}}) \n",
    "    &= \\text{Var}\\left((\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\bf{y}\\right) && \\text{Definition of the OLS estimators}  \\\\\n",
    "    &= (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\text{Var} (\\bf{y})\\left((\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\right)^\\top && \\text{Property of the variance on matrices} \\\\\n",
    "    &= (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\text{Var} (\\bf{y}) \\bf{X}\\left((\\bf{X}^\\top \\bf{X})^{-1} \\right)^\\top && \\text{Property of the transpose}  \\\\\n",
    "    &= (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\text{Var}(\\bf{X} \\bf{w} + \\bf{\\epsilon}) \\ \\bf{X}\\left((\\bf{X}^\\top \\bf{X})^{-1} \\right)^\\top && \\text{Modelization of the data}\\\\\n",
    "    &= (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\text{Var}(\\bf{\\epsilon}) \\ \\bf{X}\\left((\\bf{X}^\\top \\bf{X})^{-1} \\right)^\\top && \\text{Property of the variance (only $\\epsilon$ is a random variable)} \\\\\n",
    "    &= (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\bf{I} \\ \\bf{X}\\left((\\bf{X}^\\top \\bf{X})^{-1} \\right)^\\top && \\text{Since $\\epsilon$ is iid}\\\\\n",
    "    &= (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top  \\bf{X}\\left((\\bf{X}^\\top \\bf{X})^{-1} \\right)^\\top \\\\\n",
    "    &= \\left((\\bf{X}^\\top \\bf{X})^{-1} \\right)^\\top \\\\\n",
    "    &= (\\bf{X}^\\top \\bf{X})^{-1} && \\text{Since $\\bf{X}^\\top \\bf{X}$ is symetric and by property of the inverse of a matrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Compute the variance of the OLS estimators according to the above expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computation\n",
    "var = np.linalg.inv(np.dot(X_train.T, X_train))\n",
    "\n",
    "# Interest\n",
    "print(\"Analytical variance of the w_0 estimate: \", var[0, 0])   \n",
    "print(\"Analytical variance of the w_1 estimate: \",var[1, 1])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Are the last results make sense according to the the MC estimate previously calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Yes! The analytical estimates of the variance of the OLS estimators are approximately the same as those obtained by MC methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 OLS estimators with L2-Regularization\n",
    "\n",
    "As pointed out in slide 36, we can add a L$_2$ regularization on the weights of the linear model defined in Section 2.1. The associated estimators are known as the [Ridge or Tikhonov estimator](https://en.wikipedia.org/wiki/Tikhonov_regularization) and are defined as\n",
    "\n",
    "$$ \\hat{w}^{\\text{ridge}} := (\\bf{X}^\\top \\bf{X} + \\lambda \\bf{I})^{-1} \\bf{X}^\\top \\bf{y}$$\n",
    "\n",
    "where $\\lambda$ is a hyperparameter (see slides 37 and 38) wich control the model's capacity. \n",
    "\n",
    "**Question**: Given the above expression, complete the ridge function below to obtain the ridge estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge(X, y, lambda_hp):\n",
    "    \n",
    "    A = np.linalg.inv(np.dot(X.T, X) + lambda_hp * np.identity(X.shape[1]))\n",
    "    B = np.dot(X.T, y)\n",
    "    \n",
    "    return np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Compute the ridge estimators for different values of $\\lambda$. But first, try to understand how the ridge estimator will behave according to these 3 scenarios:\n",
    "\n",
    "1. For $\\lambda = 0$. \n",
    "2. For $\\lambda = 10^10$.\n",
    "3. How the estimators generally behave according to $\\lambda$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers**:\n",
    "\n",
    "1. The ridge estimators are equivalent to the OLS estimators.\n",
    "2. The ridge estimators should collapse.\n",
    "3. Experiment by yourself (or look at slide 37 ;))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lambda_hp = 10**3\n",
    "w_ridge = ridge(X_train, y_train, lambda_hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Make predictions on training and test sets. Are they better than the predictions made by the OLS estimators?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set\n",
    "y_hat_train = np.dot(X_train, w_ridge)\n",
    "\n",
    "# Test set\n",
    "y_hat_test = np.dot(X_test, w_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3.1 Bias of the ridge estimators\n",
    "\n",
    "**Question**: Calculate the bias of the estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = w_ridge - [w_0, w_1]\n",
    "\n",
    "print(\"Bias of w_0: \", bias[0])   # Bias of w_0\n",
    "print(\"Bias of the w_1: \", bias[1])   # Bias of w_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Compare the bias of the ridge estimators with those of the OLS. Does it make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Ridge estimators are biased. Hence, the empirical bias of those estimators should be greater than those of the OLSs. We can see that the Rigde estimators are biased. We can see that the Rigde estimators are biased. \n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbb{E}[\\hat{\\bf{w}}^{\\text{ridge}}] \n",
    "    &= \\mathbb{E}[(\\bf{X}^\\top \\bf{X} + \\lambda \\bf{I})^{-1} \\bf{X}^\\top \\bf{y}] \\\\\n",
    "    &= (\\bf{X}^\\top \\bf{X} + \\lambda \\bf{I})^{-1} \\bf{X}^\\top \\mathbb{E}[\\bf{y}] \\\\\n",
    "    &= (\\bf{X}^\\top \\bf{X} + \\lambda \\bf{I})^{-1} \\bf{X}^\\top \\mathbb{E}[\\bf{X} \\bf{w} + \\bf{\\epsilon}] \\\\\n",
    "    &= (\\bf{X}^\\top \\bf{X} + \\lambda \\bf{I})^{-1} \\bf{X}^\\top \\{ \\mathbb{E}[\\bf{X} \\bf{w}] + \\mathbb{E}[\\bf{\\epsilon}] \\} \\\\\n",
    "    &= (\\bf{X}^\\top \\bf{X} + \\lambda \\bf{I})^{-1} \\bf{X}^\\top \\{ \\bf{X} \\bf{w} + \\bf{O} \\} \\\\\n",
    "    &= (\\bf{X}^\\top \\bf{X} + \\lambda \\bf{I})^{-1} \\bf{X}^\\top \\bf{X} \\bf{w}  \\\\\n",
    "    &= (\\bf{X}^\\top \\bf{X} + \\lambda \\bf{I})^{-1} (\\bf{X}^\\top \\bf{X} + \\lambda \\bf{I} - \\lambda \\bf{I} ) \\bf{w}  \\\\\n",
    "    &= (\\bf{X}^\\top \\bf{X} + \\lambda \\bf{I})^{-1} (\\bf{X}^\\top \\bf{X} + \\lambda \\bf{I}) \\bf{w} -  (\\bf{X}^\\top \\bf{X} + \\lambda \\bf{I})^{-1}\\lambda \\bf{I} \\bf{w}  \\\\\n",
    "    &=  \\bf{w} -  \\lambda (\\bf{X}^\\top \\bf{X} + \\lambda \\bf{I})^{-1} \\bf{w}.\n",
    "\\end{align}\n",
    "\n",
    "Hence, the bias of the ridge estimator is:\n",
    "$$ \\text{Bias}(\\hat{\\bf{w}}^{\\text{ridge}})   = -  \\lambda (\\bf{X}^\\top \\bf{X} + \\lambda \\bf{I})^{-1} \\bf{w}  $$\n",
    "\n",
    "Note that under classical assumption, its empirical bias is always be greater than the one associated to the OLS estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: One question remains: how can choose the hyper parameter value $\\lambda$ of the ridge estimator? The following section will suggest an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Cross validation - Getting the optimal hyper parameter value\n",
    "\n",
    "Cross validation can be used (among other techniques) in order to perform model and hyper parameters selection. The first step is to split the train set into a (smaller) train set and validation set as shown in Slide 38 of the course. In Python, we will mostly use the train_test_split function from the Scikitlearn library to perform data splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub_train, X_validation, y_sub_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now study the performance (according to the MSE) of the ridge estimators given a grid of hyper paramater values. \n",
    "\n",
    "**Question**: Complete the cross validation procedure below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_stack_sub_train, losses_stack_train, losses_stack_validation, losses_stack_test = [], [], [], []\n",
    "\n",
    "for lambda_hp in np.arange(0, 100, 1):\n",
    "    \n",
    "    # Learn the associated parameters (w) associated with the specifc lambda\n",
    "    w_ridge_cv = ridge(X_sub_train, y_sub_train, lambda_hp)\n",
    "    \n",
    "    # Make prediction for the sub_train set and the validation set\n",
    "    y_hat_sub_train = np.dot(X_sub_train, w_ridge_cv)\n",
    "    y_hat_validation = np.dot(X_validation, w_ridge_cv)\n",
    "    \n",
    "    # Stacking statistics\n",
    "    losses_stack_sub_train.append(MSE(y_sub_train, y_hat_sub_train))\n",
    "    losses_stack_validation.append(MSE(y_validation, y_hat_validation))\n",
    "    \n",
    "    # Pay attention to the following steps !!!\n",
    "    w_ridge = ridge(X_train, y_train, lambda_hp)\n",
    "    y_hat_train = np.dot(X_train, w_ridge)\n",
    "    y_hat_test = np.dot(X_test, w_ridge)\n",
    "\n",
    "    losses_stack_train.append(MSE(y_train, y_hat_train))\n",
    "    losses_stack_test.append(MSE(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now select to optimal hyperparameter $\\lambda$. The optimal value will minimize the loss function on the validation set (as presented on Slides 31 and 44). Note that the selection of the optimal hyper parameter will never be based on the its behavior on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap(\"tab10\")\n",
    "\n",
    "plt.plot(losses_stack_test, label='Test', color=cmap(2))\n",
    "plt.plot(losses_stack_validation, label='Validation', color=cmap(0))\n",
    "plt.plot(losses_stack_sub_train, label='Training', color=cmap(1))\n",
    "\n",
    "plt.axvline(x=np.argmin(losses_stack_validation),color='red', label='Optimal regularization')\n",
    "\n",
    "plt.xlabel('$\\lambda$ regularization')\n",
    "plt.ylabel('MSE')\n",
    "leg = plt.gca().legend(loc='center left', bbox_to_anchor=(1, .8))\n",
    "leg.get_frame().set_alpha(0)\n",
    "print('The optimal regularization is', np.argmin(losses_stack_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all this work, it could be interesting to compare the ridge estimator to the OLS estimator on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('TRAIN: OLS estimators:   ', losses_stack_train[0])\n",
    "print('TRAIN: ridge estimators: ', losses_stack_train[np.argmin(losses_stack_validation)])\n",
    "\n",
    "print('\\nTEST: OLS estimators:   ', losses_stack_test[0])\n",
    "print('TEST: ridge estimators: ', losses_stack_test[np.argmin(losses_stack_validation)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Take home messages - Final remarks\n",
    "\n",
    "1. Model with high capacity don't necessary have a lower generalization error.\n",
    "2. Unbiased estimator don't necessary have a lower generalization error.\n",
    "3. In the specific case where the loss function is based on the MSE, we can decompose the generalization error with respect to the bias and the variance of the associated estimators.\n",
    "4. Cross validation procedure can be tricky! Be sure to understand the code snippet in Section 2.3!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
