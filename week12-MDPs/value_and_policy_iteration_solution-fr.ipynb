{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH80629\n",
    "# Semaine 12 - Prise de décision séquentielle I - Exercices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auteur: Massimo Caccia massimo.p.caccia@gmail.com <br>\n",
    "\n",
    "Ces exercices ont été adaptés à partir de : https://github.com/lazyprogrammer/machine_learning_examples/tree/master/rl <br>\n",
    "ainsi que: https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction\n",
    "\n",
    "Avant de discuter des algorithmes de value iteration et policy iteration, nous allons tester nos connaissances des processus de décision markovien (Markov Decision Process ou MDP). <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Tic-Tac-Toe\n",
    "\n",
    "Prenons l'exemple du&nbsp;: Tic-Tac-Toe (aussi appelé \"morpion\" et \"oxo\"). \n",
    "\n",
    "Définition: Deux joueurs s'affrontent. Ils doivent remplir chacun à leur tour une case de la grille avec le symbole qui leur est attribué : O ou X. Le gagnant est celui qui arrive à aligner trois symboles identiques, horizontalement, verticalement ou en diagonale. (définition venant de https://fr.wikipedia.org/wiki/Tic-tac-toe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://bjc.edc.org/bjc-r/img/3-lists/TTT1_img/Three%20States%20of%20TTT.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://bjc.edc.org/bjc-r/img/3-lists/TTT1_img/Three%20States%20of%20TTT.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Si vous vouliez développer un agent pour ce jeu vous devriez d'abord spécifier comment on modélise l'environement. Qu'utiliseriez-vous comme états, actions, fonction de transition et récompenses?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Réponse:**<br>\n",
    "L'**espace des états** est une matrice 3x3 ou un vecteur de taille 9 qui indique si une case est: a) vide, b) prise par un X ou c) prise par un O. <br>\n",
    "\n",
    "Les **actions** sont les 9 cases où vous pouvez jouer. Il y a donc 9 actions possibles en tout. Par contre, toutes les actions ne sont pas toujours disponibles. Notamment, seules les actions ajoutant sur une case vide le sont. <br>\n",
    "\n",
    "Pour la **fonction de récompense** on pourrait utiliser +1 si vous gagnez, -1 si vous perdez, et 0 pour une partie nulle.\n",
    "\n",
    "La **fonction de transition** est donnée par la stratégie de l'autre joueur. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Système de recommandation\n",
    "\n",
    "**Question:** La semaine dernière, nous avons discuté des systèmes de recommandation. Comment faire pour modéliser un système (agent) qui suggère des recommandations en utilisant un MDP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Réponse:**\n",
    "\n",
    "**États:** Vous voudriez encoder dans vos états les préférences de l'utilisateur. Une façon de faire, serait que l'état soit la liste des items déjà consommés par l'utilisateur. \n",
    "\n",
    "**Actions:** l'item à recommander (item 1, item 2, ... item n). Le nombre d'actions est donc le nombre d'items dans le catalogue.\n",
    "\n",
    "**Récompense:** +1 si l'utilisateur consomme l'item et -1 si non. (ce n'est qu'un exemple, on pourrait aussi utiliser une récompense plus complexe.)\n",
    "\n",
    "**Probabilités de transition:** ça va dépendre de votre utilisateur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces exercices testent votre compréhension de l'algorithme Value iteration.\n",
    "\n",
    "L'algorithme complet se trouve à la [diapo 46](http://www.cs.toronto.edu/~lcharlin/courses/80-629/slides_rl.pdf). <br>\n",
    "\n",
    "Nous utiliserons l'algorithme pour résoudre un mode grille (Gridworld) similaire à celui présenté à la diapo 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Mise en place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘gridWorldGame.py’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "\n",
    "!wget -nc https://raw.githubusercontent.com/lcharlin/80-629/master/week12-MDPs/gridWorldGame.py\n",
    "    \n",
    "import numpy as np\n",
    "from gridWorldGame import standard_grid, negative_grid, print_values, print_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques variables utiles dans notre implémentation. <br>\n",
    "`SMALL_ENOUGH` pour déterminer quand l'algorithme a convergé<br>\n",
    "`GAMMA` est le facteur d'actualisation $\\gamma$ (discount factor)  dans les diapos (diapo 36) <br>\n",
    "`ALL_POSSIBLE_ACTIONS` sont les actions disponibles dans l'environement (voir diapo 12): haut (up), bas (down), right (droite), left (gauche).<br>\n",
    "`NOISE_PROB` défini la stochasticité de l'environement. En d'autres mots, c'est la probabilité qu'une action résulte en un état autre que celui voulu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_ENOUGH = 1e-3 # \n",
    "GAMMA = 0.9         # \n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R') # Up, Down, Left, Right\n",
    "NOISE_PROB = 0.1    # Probabilité que l'agent n'atteigne pas « l'état voulu »"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons écrit une classe pour obtenir un environement en grille (GridWorld). <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "les récompenses:\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.10|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n"
     ]
    }
   ],
   "source": [
    "grid = standard_grid(noise_prob=NOISE_PROB)\n",
    "print(\"les récompenses:\")\n",
    "print_values(grid.rewards, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a trois états absorbants: (0,3),(1,3) et (1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous alors d'abord définir une politique $\\pi$ aléatoire. <br>\n",
    "\n",
    "Rappel: une politique est une fonction qui nous décide de l'action à performer dans chaque état $\\pi : S \\rightarrow A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politique initiale:\n",
      "---------------------------\n",
      "  D  |  R  |  U  | N/A |\n",
      "---------------------------\n",
      "  R  | N/A |  D  | N/A |\n",
      "---------------------------\n",
      "  D  |  U  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "policy = {}\n",
    "for s in grid.actions.keys():\n",
    "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "# politique initiale\n",
    "print(\"politique initiale:\")\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que nous n'avons pas besoin de définir la politique sur les états absorbants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous initialisons ensuite la fonction de valeur de manière aléatoire pour tous les états sauf les états absorbants qui auront une valeur de 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      " 0.44| 0.19| 0.96| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.62| 0.00|\n",
      "---------------------------\n",
      " 0.78| 0.79| 0.28| 0.27|\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234) # pour obtenir les mêmes résultats d'une fois à l'autre\n",
    "\n",
    "V = {}\n",
    "states = grid.all_states()\n",
    "for s in states:\n",
    "    # V[s] = 0\n",
    "    if s in grid.actions:\n",
    "        V[s] = np.random.random()\n",
    "    else:\n",
    "        # terminal state\n",
    "        V[s] = 0\n",
    "\n",
    "# visualiser la fonction de valeur\n",
    "print_values(V, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 L'algorithme de Value iteration - code à compléter\n",
    "\n",
    "Rappel: Pour Value iteration, l'idée est d'optimiser la fonction de valeur. Une fois obtenue on peut obtenir la meilleure politique pour chaque état. <br>\n",
    "\n",
    "Nous vous demandons de compléter l'algorithme de Value iteration.<br>\n",
    "\n",
    "À chaque itération, la valeur $V(s)$ de chaque état $s$ doit être mise à jour avec la formule suivante:\n",
    "\n",
    "$$\n",
    "V(s) = \\underset{a}{max}\\big\\{ \\sum_{s'}  p(s'|s,a)(r + \\gamma*V(s') \\big\\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itération VI 0: \n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.80| 0.89| 0.80|\n",
      "\n",
      "\n",
      "\n",
      "\t le plus grand changement de valeur est : 0.000000 \n",
      "\n",
      "\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.80| 0.89| 0.80|\n"
     ]
    }
   ],
   "source": [
    "iteration=0\n",
    "while True:\n",
    "    print(\"Itération VI %d: \" % iteration)\n",
    "    print_values(V, grid)\n",
    "    print(\"\\n\\n\")\n",
    "  \n",
    "    biggest_change = 0\n",
    "    for s in states:\n",
    "        old_v = V[s]\n",
    "       \n",
    "        # pour chaque état qui n'est pas un état absorbant\n",
    "        if s in policy:\n",
    "            new_v = float('-inf')\n",
    "\n",
    "            # pour chaque action\n",
    "            for a in ALL_POSSIBLE_ACTIONS:\n",
    "                grid.set_state(s)\n",
    "                r = grid.move(a)\n",
    "                sprime = grid.current_state()\n",
    "                #  - calculer: [s] = max[a]{ sum[s',r] { p(s',r|s,a)[r + gamma*V[s']] } }\n",
    "                v = r + GAMMA * V[sprime]\n",
    "                if v > new_v: # est-ce la meilleure action pour l'instant?\n",
    "                    new_v = v\n",
    "            V[s] = new_v\n",
    "            biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "    print('\\t le plus grand changement de valeur est : %f \\n\\n' % biggest_change)\n",
    "    if biggest_change < SMALL_ENOUGH:\n",
    "        break\n",
    "    iteration+=1\n",
    "print_values(V, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons optimisé la fonction de valeur, nous pouvons obtenir (décoder) la politique optimale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "deterministic_grid = standard_grid(noise_prob=0.)\n",
    "\n",
    "for s in policy.keys():\n",
    "    best_a = None\n",
    "    best_value = float('-inf')\n",
    "    # on cherche la meilleure action pour chaque état\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "        deterministic_grid.set_state(s)\n",
    "        r = deterministic_grid.move(a)\n",
    "        v = r + GAMMA * V[deterministic_grid.current_state()]\n",
    "        if v > best_value:\n",
    "            best_value = v\n",
    "            best_a = a\n",
    "    policy[s] = best_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant visualiser notre politique pour s'assurer qu'elle semble correcte. Notamment, la politique devrait « aller » vers le coin en haut à droite de la grille puisque c'est là qu'il y a la seule récompense positive (+1.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valeurs:\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.80| 0.89| 0.80|\n",
      "\n",
      "politique:\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "print(\"valeurs:\")\n",
    "print_values(V, grid)\n",
    "print(\"\\npolitique:\")\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous vous demandons maintenant de **compléter l'algorithme de policy iteration**. <br>\n",
    "Vous pouvez trouvez les détails de l'algorithme à la diapo 47. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous commençons par définir une politique aléatoire. <br>\n",
    "Remember that a policy maps states to actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politique initiale:\n",
      "---------------------------\n",
      "  R  |  D  |  L  | N/A |\n",
      "---------------------------\n",
      "  L  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  L  |  R  |  L  |  D  |\n"
     ]
    }
   ],
   "source": [
    "policy = {}\n",
    "for s in grid.actions.keys():\n",
    "    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "# initial policy\n",
    "print(\"politique initiale:\")\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et, nous initialisons aussi la fonction de valeur de manière aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      " 0.44| 0.19| 0.96| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.62| 0.00|\n",
      "---------------------------\n",
      " 0.78| 0.79| 0.28| 0.27|\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "# on initialize la fonction de valeur V(s)\n",
    "V = {}\n",
    "states = grid.all_states()\n",
    "for s in states:\n",
    "    if s in grid.actions:\n",
    "        V[s] = np.random.random()\n",
    "    else:\n",
    "        # état terminal\n",
    "        V[s] = 0\n",
    "\n",
    "# visualisation de la fonction de valeur\n",
    "print_values(V, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Policy iteration - code à compléter\n",
    "\n",
    "Vous avez maintenant à compléter l'algorithme Policy iteration.<br>\n",
    "Rappel: l'algorithme fonctionne en deux étapes. <br>\n",
    "\n",
    "1. on évalue la politique actuelle (*policy evaluation*) en calculant sa fonction de valeur:\n",
    "\n",
    "$$\n",
    "V^\\pi(s) =  \\sum_{s'}  p(s'|s,\\pi(s))(r + \\gamma*V^\\pi(s') \n",
    "$$\n",
    "(Cette partie de l'algorithme vous est donnée.) <br>\n",
    "\n",
    "2. on améliore la politique (*policy improvement*). Pour chaque état on prend l'action qui donne la meilleure fonction de valeur:\n",
    "\n",
    "$$\n",
    "\\pi'(s) = \\underset{a}{arg max}\\big\\{ \\sum_{s'}  p(s'|s,a)(r + \\gamma*V^\\pi(s') \\big\\}\n",
    "$$\n",
    "\n",
    "(Vous devez compléter cette partie du code.)<br>\n",
    "\n",
    "L'algorithme itére ces deux étapes jusqu'à convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values (iteration 0)\n",
      "---------------------------\n",
      " 0.44| 0.19| 0.96| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.62| 0.00|\n",
      "---------------------------\n",
      " 0.78| 0.79| 0.28| 0.27|\n",
      "policy (iteration 0)\n",
      "---------------------------\n",
      "  R  |  D  |  L  | N/A |\n",
      "---------------------------\n",
      "  L  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  L  |  R  |  L  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 1)\n",
      "---------------------------\n",
      " 0.00| 0.01| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.01|\n",
      "policy (iteration 1)\n",
      "---------------------------\n",
      "  R  |  U  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  L  |  R  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 2)\n",
      "---------------------------\n",
      " 0.00| 0.00| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "policy (iteration 2)\n",
      "---------------------------\n",
      "  R  |  R  |  U  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 3)\n",
      "---------------------------\n",
      " 0.01| 0.01| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.00| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.01| 0.01| 0.01|\n",
      "policy (iteration 3)\n",
      "---------------------------\n",
      "  R  |  U  |  U  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  U  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 4)\n",
      "---------------------------\n",
      " 0.01| 0.01| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.00| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.01| 0.01| 0.01|\n",
      "policy (iteration 4)\n",
      "---------------------------\n",
      "  R  |  U  |  U  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  U  |  U  |  U  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 5)\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "policy (iteration 5)\n",
      "---------------------------\n",
      "  R  |  U  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  U  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 6)\n",
      "---------------------------\n",
      " 0.00| 0.00| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.89| 0.80|\n",
      "policy (iteration 6)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  R  |  D  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 7)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "policy (iteration 7)\n",
      "---------------------------\n",
      "  D  |  R  |  U  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  U  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 8)\n",
      "---------------------------\n",
      " 0.00| 0.01| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.01| 0.01|\n",
      "policy (iteration 8)\n",
      "---------------------------\n",
      "  R  |  U  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  D  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 9)\n",
      "---------------------------\n",
      " 0.00| 0.00| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.01| 0.89| 0.80|\n",
      "policy (iteration 9)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 10)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.80| 0.89| 0.80|\n",
      "policy (iteration 10)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  D  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 11)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.65| 0.89| 0.80|\n",
      "policy (iteration 11)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 12)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.80| 0.89| 0.80|\n",
      "policy (iteration 12)\n",
      "---------------------------\n",
      "  R  |  D  |  R  | N/A |\n",
      "---------------------------\n",
      "  L  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 13)\n",
      "---------------------------\n",
      " 0.01| 0.01| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.80| 0.89| 0.80|\n",
      "policy (iteration 13)\n",
      "---------------------------\n",
      "  D  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  D  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  U  |  D  |  U  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 14)\n",
      "---------------------------\n",
      "-0.00| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      "-0.00| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      "-0.00|-0.00|-0.00|-1.00|\n",
      "policy (iteration 14)\n",
      "---------------------------\n",
      "  R  |  R  |  U  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  L  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 15)\n",
      "---------------------------\n",
      " 0.01| 0.01| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.00| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.01| 0.01| 0.01|\n",
      "policy (iteration 15)\n",
      "---------------------------\n",
      "  R  |  U  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 16)\n",
      "---------------------------\n",
      " 0.00| 0.00| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.80| 0.89| 0.00|\n",
      "policy (iteration 16)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 17)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.80| 0.89| 0.80|\n",
      "policy (iteration 17)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  R  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 18)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72|-0.00|-0.00|-0.00|\n",
      "policy (iteration 18)\n",
      "---------------------------\n",
      "  U  |  R  |  U  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  L  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 19)\n",
      "---------------------------\n",
      " 0.01| 0.01| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.00| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.01| 0.01| 0.01|\n",
      "policy (iteration 19)\n",
      "---------------------------\n",
      "  R  |  U  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 20)\n",
      "---------------------------\n",
      " 0.00| 0.00| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.80| 0.89|-1.00|\n",
      "policy (iteration 20)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 21)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.80| 0.89| 0.80|\n",
      "policy (iteration 21)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 22)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.80| 0.89| 0.80|\n",
      "policy (iteration 22)\n",
      "---------------------------\n",
      "  U  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 23)\n",
      "---------------------------\n",
      " 0.01| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.80| 0.89| 0.80|\n",
      "policy (iteration 23)\n",
      "---------------------------\n",
      "  U  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 24)\n",
      "---------------------------\n",
      " 0.01| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.80| 0.89| 0.80|\n",
      "policy (iteration 24)\n",
      "---------------------------\n",
      "  L  |  R  |  U  | N/A |\n",
      "---------------------------\n",
      "  D  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 25)\n",
      "---------------------------\n",
      " 0.01| 0.01| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.00| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.01| 0.01| 0.01|\n",
      "policy (iteration 25)\n",
      "---------------------------\n",
      "  U  |  U  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  L  | N/A |\n",
      "---------------------------\n",
      "  R  |  U  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 26)\n",
      "---------------------------\n",
      " 0.01| 0.01| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.01| 0.01|\n",
      "policy (iteration 26)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 27)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.80| 0.89| 0.00|\n",
      "policy (iteration 27)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  U  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 28)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.80| 0.89|-1.00|\n",
      "policy (iteration 28)\n",
      "---------------------------\n",
      "  R  |  D  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 29)\n",
      "---------------------------\n",
      " 0.01| 0.01| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.80| 0.89| 0.80|\n",
      "policy (iteration 29)\n",
      "---------------------------\n",
      "  R  |  R  |  D  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 30)\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "policy (iteration 30)\n",
      "---------------------------\n",
      "  R  |  U  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  L  | N/A |\n",
      "---------------------------\n",
      "  R  |  U  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 31)\n",
      "---------------------------\n",
      " 0.00| 0.00| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "policy (iteration 31)\n",
      "---------------------------\n",
      "  R  |  U  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  D  |  U  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values (iteration 32)\n",
      "---------------------------\n",
      " 0.01| 0.01| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.01| 0.89| 0.01|\n",
      "policy (iteration 32)\n",
      "---------------------------\n",
      "  R  |  R  |  U  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 33)\n",
      "---------------------------\n",
      " 0.01| 0.01| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.00| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.01| 0.01| 0.01|\n",
      "policy (iteration 33)\n",
      "---------------------------\n",
      "  R  |  U  |  U  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  U  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 34)\n",
      "---------------------------\n",
      " 0.01| 0.01| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.01|-1.00|\n",
      "policy (iteration 34)\n",
      "---------------------------\n",
      "  R  |  U  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  L  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 35)\n",
      "---------------------------\n",
      " 0.00| 0.00| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "policy (iteration 35)\n",
      "---------------------------\n",
      "  L  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  L  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 36)\n",
      "---------------------------\n",
      " 0.00| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.89| 0.00|\n",
      "policy (iteration 36)\n",
      "---------------------------\n",
      "  U  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 37)\n",
      "---------------------------\n",
      " 0.00| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.80| 0.89| 0.80|\n",
      "policy (iteration 37)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 38)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.80| 0.89| 0.80|\n",
      "policy (iteration 38)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 39)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.80| 0.89| 0.80|\n",
      "policy (iteration 39)\n",
      "---------------------------\n",
      "  R  |  R  |  U  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 40)\n",
      "---------------------------\n",
      " 0.01| 0.01| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.00| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.01| 0.01| 0.01|\n",
      "policy (iteration 40)\n",
      "---------------------------\n",
      "  D  |  U  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  U  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 41)\n",
      "---------------------------\n",
      " 0.00| 0.00| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.89| 0.00|\n",
      "policy (iteration 41)\n",
      "---------------------------\n",
      "  U  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  U  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 42)\n",
      "---------------------------\n",
      " 0.01| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.01| 0.89| 0.80|\n",
      "policy (iteration 42)\n",
      "---------------------------\n",
      "  L  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  D  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  D  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 43)\n",
      "---------------------------\n",
      " 0.00| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.01| 0.89| 0.80|\n",
      "policy (iteration 43)\n",
      "---------------------------\n",
      "  R  |  R  |  U  | N/A |\n",
      "---------------------------\n",
      "  D  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 44)\n",
      "---------------------------\n",
      " 0.01| 0.01| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.00| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.01| 0.01| 0.01|\n",
      "policy (iteration 44)\n",
      "---------------------------\n",
      "  R  |  U  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  U  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 45)\n",
      "---------------------------\n",
      " 0.00| 0.00| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.89| 0.00|\n",
      "policy (iteration 45)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  D  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 46)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.89| 0.80|\n",
      "policy (iteration 46)\n",
      "---------------------------\n",
      "  U  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 47)\n",
      "---------------------------\n",
      " 0.01| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.80| 0.89| 0.80|\n",
      "policy (iteration 47)\n",
      "---------------------------\n",
      "  R  |  L  |  U  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  R  |  U  |  U  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 48)\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.01| 0.01|-1.00|\n",
      "policy (iteration 48)\n",
      "---------------------------\n",
      "  L  |  L  |  D  | N/A |\n",
      "---------------------------\n",
      "  D  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  U  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 49)\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "policy (iteration 49)\n",
      "---------------------------\n",
      "  R  |  U  |  U  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  L  | N/A |\n",
      "---------------------------\n",
      "  R  |  R  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 50)\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "policy (iteration 50)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  D  | N/A |  L  | N/A |\n",
      "---------------------------\n",
      "  R  |  U  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 51)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "policy (iteration 51)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  U  |  U  |  R  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 52)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.00| 0.89| 0.01|\n",
      "policy (iteration 52)\n",
      "---------------------------\n",
      "  R  |  R  |  U  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 53)\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "policy (iteration 53)\n",
      "---------------------------\n",
      "  R  |  U  |  U  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  U  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 54)\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "policy (iteration 54)\n",
      "---------------------------\n",
      "  L  |  U  |  L  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  D  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 55)\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "policy (iteration 55)\n",
      "---------------------------\n",
      "  R  |  U  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  L  | N/A |\n",
      "---------------------------\n",
      "  R  |  U  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 56)\n",
      "---------------------------\n",
      " 0.00| 0.00| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "policy (iteration 56)\n",
      "---------------------------\n",
      "  R  |  R  |  U  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  U  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 57)\n",
      "---------------------------\n",
      " 0.00| 0.01| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.01| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "policy (iteration 57)\n",
      "---------------------------\n",
      "  R  |  U  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  U  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values (iteration 58)\n",
      "---------------------------\n",
      " 0.00| 0.00| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.89| 0.00|\n",
      "policy (iteration 58)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  D  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 59)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.01| 0.01| 0.01|\n",
      "policy (iteration 59)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  L  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 60)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.65| 0.89| 0.01|\n",
      "policy (iteration 60)\n",
      "---------------------------\n",
      "  R  |  R  |  L  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 61)\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "policy (iteration 61)\n",
      "---------------------------\n",
      "  L  |  U  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  L  | N/A |\n",
      "---------------------------\n",
      "  R  |  U  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 62)\n",
      "---------------------------\n",
      " 0.01| 0.01| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      "-0.00|-0.00| 0.00| 0.00|\n",
      "policy (iteration 62)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 63)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.80| 0.89| 0.00|\n",
      "policy (iteration 63)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 64)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.80| 0.89| 0.80|\n",
      "policy (iteration 64)\n",
      "---------------------------\n",
      "  L  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 65)\n",
      "---------------------------\n",
      " 0.01| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.01| 0.80| 0.89| 0.80|\n",
      "policy (iteration 65)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  L  |  U  |  U  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 66)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.89|-1.00|\n",
      "policy (iteration 66)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 67)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.80| 0.89| 0.80|\n",
      "policy (iteration 67)\n",
      "---------------------------\n",
      "  R  |  R  |  U  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 68)\n",
      "---------------------------\n",
      "-0.01|-0.01|-0.01| 0.00|\n",
      "---------------------------\n",
      "-0.00| 0.00|-0.01| 0.00|\n",
      "---------------------------\n",
      "-0.00|-0.01|-0.01|-0.01|\n",
      "policy (iteration 68)\n",
      "---------------------------\n",
      "  D  |  L  |  R  | N/A |\n",
      "---------------------------\n",
      "  D  | N/A |  D  | N/A |\n",
      "---------------------------\n",
      "  D  |  L  |  D  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 69)\n",
      "---------------------------\n",
      "-0.00|-0.00| 1.10| 0.00|\n",
      "---------------------------\n",
      "-0.00| 0.00|-0.00| 0.00|\n",
      "---------------------------\n",
      "-0.00|-0.00|-0.00|-0.00|\n",
      "policy (iteration 69)\n",
      "---------------------------\n",
      "  D  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  L  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  D  |  L  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 70)\n",
      "---------------------------\n",
      "-0.00| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      "-0.00| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      "-0.00|-0.00|-0.00|-0.00|\n",
      "policy (iteration 70)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  L  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 71)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.65| 0.89|-0.00|\n",
      "policy (iteration 71)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  D  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 72)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.80| 0.89| 0.00|\n",
      "policy (iteration 72)\n",
      "---------------------------\n",
      "  D  |  R  |  U  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 73)\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "policy (iteration 73)\n",
      "---------------------------\n",
      "  R  |  U  |  D  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  L  | N/A |\n",
      "---------------------------\n",
      "  D  |  U  |  R  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 74)\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "policy (iteration 74)\n",
      "---------------------------\n",
      "  R  |  D  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  D  | N/A |\n",
      "---------------------------\n",
      "  U  |  U  |  R  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 75)\n",
      "---------------------------\n",
      " 0.00| 0.00| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "policy (iteration 75)\n",
      "---------------------------\n",
      "  L  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  U  |  R  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 76)\n",
      "---------------------------\n",
      " 0.00| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "policy (iteration 76)\n",
      "---------------------------\n",
      "  U  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  U  |  U  |  D  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 77)\n",
      "---------------------------\n",
      " 0.00| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.89| 0.00|\n",
      "policy (iteration 77)\n",
      "---------------------------\n",
      "  L  |  D  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  D  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 78)\n",
      "---------------------------\n",
      " 0.00| 0.00| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.80| 0.89| 0.80|\n",
      "policy (iteration 78)\n",
      "---------------------------\n",
      "  U  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 79)\n",
      "---------------------------\n",
      " 0.00| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.80| 0.89| 0.80|\n",
      "policy (iteration 79)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  D  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  R  |  U  |  U  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 80)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.65| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.80| 0.89|-1.00|\n",
      "policy (iteration 80)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  R  |  D  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 81)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.80| 0.89| 0.80|\n",
      "policy (iteration 81)\n",
      "---------------------------\n",
      "  R  |  R  |  R  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n",
      "values (iteration 82)\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.80| 0.89| 0.80|\n",
      "policy (iteration 82)\n",
      "---------------------------\n",
      "  R  |  R  |  L  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iteration=0\n",
    "# on répète jusqu'à convergence de la politique\n",
    "while True:\n",
    "    print(\"values (iteration %d)\" % iteration)\n",
    "    print_values(V, grid)\n",
    "    print(\"policy (iteration %d)\" % iteration)\n",
    "    print_policy(policy, grid)\n",
    "    print('\\n\\n')\n",
    "\n",
    "    # 1. on évalue la politique actuelle (policy evaluation)\n",
    "    # cette implémentation procède à plusieurs itérations d'évaluation\n",
    "    # ce n'est pas tout à fait la version de l'algorithme présentée dans les diapos\n",
    "    # qui n'en fait qu'une.\n",
    "    while True:\n",
    "        biggest_change = 0\n",
    "        for s in states:\n",
    "            old_v = V[s]\n",
    "\n",
    "            # V(s) only has value if it's not a terminal state\n",
    "            if s in policy:\n",
    "                a = policy[s]\n",
    "                grid.set_state(s)\n",
    "                r = grid.move(a) # reward\n",
    "                sprime = grid.current_state() # s' \n",
    "                V[s] = r + GAMMA * V[sprime]\n",
    "            biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "        if biggest_change < SMALL_ENOUGH:\n",
    "            break\n",
    "\n",
    "    #2. on améliore la politique (policy improvement)\n",
    "    is_policy_converged = True\n",
    "    for s in states:\n",
    "        if s in policy:\n",
    "            old_a = policy[s]\n",
    "            new_a = None\n",
    "            best_value = float('-inf')\n",
    "            # on trouve la meilleure action \n",
    "            for a in ALL_POSSIBLE_ACTIONS:\n",
    "                grid.set_state(s)\n",
    "                r = grid.move(a)\n",
    "                sprime = grid.current_state() \n",
    "                v = r + GAMMA * V[sprime]\n",
    "                if v > best_value:\n",
    "                    best_value = v\n",
    "                    new_a = a\n",
    "            if new_a is None: \n",
    "                print('problem')\n",
    "            policy[s] = new_a\n",
    "            if new_a != old_a:\n",
    "                is_policy_converged = False\n",
    "\n",
    "    if is_policy_converged:\n",
    "        break\n",
    "    iteration+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant visualiser notre politique et vérifier si elle est correcte. Notamment, value iteration et policy iteration devrait donner les mêmes résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fonction de valeur finale:\n",
      "---------------------------\n",
      " 0.89| 0.99| 1.10| 0.00|\n",
      "---------------------------\n",
      " 0.80| 0.00| 0.99| 0.00|\n",
      "---------------------------\n",
      " 0.72| 0.80| 0.89| 0.80|\n",
      "\n",
      "politique finale:\n",
      "---------------------------\n",
      "  R  |  R  |  L  | N/A |\n",
      "---------------------------\n",
      "  U  | N/A |  U  | N/A |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "print(\"fonction de valeur finale:\")\n",
    "print_values(V, grid)\n",
    "print(\"\\npolitique finale:\")\n",
    "print_policy(policy, grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
